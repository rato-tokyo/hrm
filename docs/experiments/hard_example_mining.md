# ASHEM: Adaptive Supervision via Hard Example Mining

**å®Ÿé¨“æ—¥**: 2025-12-12
**å®Ÿé¨“ã‚³ãƒ¼ãƒ‰**: colab2.py
**ãƒ‡ãƒã‚¤ã‚¹**: NVIDIA L4 GPU (Google Colab)
**ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: LASH (Layered Adaptive Supervision Hierarchy)

---

## ğŸ“‹ å®Ÿé¨“æ¦‚è¦

### ç›®çš„

ASHEMè¨“ç·´æˆ¦ç•¥ï¼ˆHard Example Mining + Two-Stage Inferenceï¼‰ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã™ã‚‹ã€‚

### ä»®èª¬

- **Phase 1**: 2å±¤ãƒ¢ãƒ‡ãƒ«ã§é€šå¸¸è¨“ç·´ â†’ ä½ä¿¡é ¼åº¦ã‚µãƒ³ãƒ—ãƒ«ï¼ˆHard examplesï¼‰ã‚’åé›†
- **Phase 2**: ä¸Šä½2å±¤ã‚’è¿½åŠ  â†’ Hard examplesã®ã¿ã§è¨“ç·´
- **æ¨è«–**: ä¿¡é ¼åº¦ã«å¿œã˜ã¦Layer 2ã¾ãŸã¯Layer 4ã§æ¨è«–ï¼ˆEarly Exitï¼‰

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**:
- Hard examplesã®æ€§èƒ½ãŒå¤§å¹…ã«æ”¹å–„
- å…¨ä½“ã®ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›

### å®Ÿé¨“è¨­è¨ˆ

**Phase 1: 2å±¤ãƒ¢ãƒ‡ãƒ«è¨“ç·´**
- ãƒ‡ãƒ¼ã‚¿: WikiText-2 (10K samples)
- Layers: 2
- Patience: 1

**Confidence Thresholdè‡ªå‹•èª¿æ•´**
- Target ratio: 50% (Hard examplesæ¯”ç‡)
- æ–¹æ³•: Quantile-based threshold computation

**Phase 2: Hard examplesè¨“ç·´**
- Layers: 4 (2å±¤è¿½åŠ )
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿: Hard examplesã®ã¿
- æ—¢å­˜å±¤: å‡çµ
- å­¦ç¿’ç‡: 1e-4 (Phase 1ã®0.1å€)
- Patience: 3
- Early StoppingåŸºæº–: **Val PPL** (é‡è¦: Val Accã§ã¯ãªã)

---

## ğŸ“Š å®Ÿé¨“çµæœ

### Phase 1: 2å±¤ãƒ¢ãƒ‡ãƒ«è¨“ç·´

| Metric | Value |
|--------|-------|
| Best Acc | **16.30%** |
| Best PPL | **975.07** |
| Time | 22.43s |
| Best Epoch | 3 |

**Early Stopping**: Epoch 4ã§åœæ­¢ï¼ˆPatience=1ï¼‰

### Confidence Thresholdè‡ªå‹•èª¿æ•´

| Parameter | Value |
|-----------|-------|
| Target ratio | 50% |
| Computed threshold | **0.1648** |
| Collected samples | 32,000 / 64,000 |
| Actual ratio | **50.0%** âœ… |
| Average confidence | 0.0764 |

**æˆåŠŸ**: æ­£ç¢ºã«50%ã®Hard examplesã‚’åé›†

### Phase 1 Hard Examplesè©•ä¾¡

| Metric | Value |
|--------|-------|
| Overall Val PPL | 975.07 |
| **Hard PPL** | **2599.93** |
| Difference | **+1624.86 (+166.7%)** |

**Hard examplesã¯2.7å€é›£ã—ã„**: é€šå¸¸ã‚µãƒ³ãƒ—ãƒ«ã‚ˆã‚Šã¯ã‚‹ã‹ã«é«˜ã„PPL

### Phase 2: Hard Examplesè¨“ç·´

#### è¨“ç·´éç¨‹

| Epoch | Train PPL | Val PPL | Val Acc | Hard PPL | Status |
|-------|-----------|---------|---------|----------|--------|
| 1 | 2911.96 | 948.38 | 15.70% | 1711.29 | âœ“ Best |
| 2 | 1458.12 | 864.93 | 15.63% | 1203.26 | âœ“ Best |
| 3 | 1108.50 | 838.04 | 15.53% | 971.69 | âœ“ Best |
| 4 | 928.78 | 828.73 | 15.43% | 837.36 | âœ“ Best |
| 5 | 815.50 | 825.87 | 15.34% | 745.32 | âœ“ Best |
| 6 | 733.68 | 824.48 | 15.30% | 675.41 | âœ“ Best |
| 7 | 669.43 | 823.98 | 15.28% | 618.81 | âœ“ Best |
| 8 | 616.27 | **823.89** | 15.27% | **571.10** | âœ“ Best |
| 9 | 570.76 | 825.47 | 15.26% | 529.77 | âœ— No improvement (1/3) |
| 10 | 530.89 | 827.32 | 15.28% | 493.29 | âœ— No improvement (2/3) |
| 11 | 495.41 | 831.21 | 15.26% | 460.66 | âœ— No improvement (3/3) |

**Early Stopping**: Epoch 11ã§åœæ­¢ï¼ˆBest: Epoch 8ï¼‰

#### Phase 2çµæœ

| Metric | Value |
|--------|-------|
| Best Val PPL | **823.89** |
| Best Hard PPL | **571.10** |
| Time | 74.06s |

### æœ€çµ‚è©•ä¾¡: Two-Stage Inference

| Metric | Value |
|--------|-------|
| Accuracy | **15.27%** |
| PPL | **823.89** |
| Shallow ratio (Layer 2) | **72.0%** |
| Deep ratio (Layer 4) | **28.0%** |
| **Compute cost** | **63.98%** of full model |

**åŠ¹ç‡æ€§**: 36%ã®è¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›

---

## ğŸ” è©³ç´°åˆ†æ

### 1. **Hard Examplesæ€§èƒ½ã®åŠ‡çš„æ”¹å–„** â­

```
Phase 1 Hard PPL:  2599.93
Phase 2 Hard PPL:   571.10
Improvement:       +2028.83 (+78.0%)
```

**é©šç•°çš„ãªçµæœ**:
- Hard examplesã®PPLãŒ**78%å‰Šæ¸›**
- **4.5å€ä»¥ä¸Šã®æ€§èƒ½å‘ä¸Š**
- Hard example miningã®æœ‰åŠ¹æ€§ã‚’è¨¼æ˜

### 2. **Overallæ€§èƒ½ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**

```
                    Accuracy    PPL
Phase 1 (2-layer):   16.30%   975.07
Two-stage:           15.27%   823.89
Change:              -1.04%   -15.5%
```

**è§£é‡ˆ**:
- Accuracy: ã‚ãšã‹ã«ä½ä¸‹ï¼ˆ-1.04%ï¼‰
- PPL: æ”¹å–„ï¼ˆ-15.5%ï¼‰
- ã“ã‚Œã¯**æ­£å¸¸ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**

**ç†ç”±**:
- Hard examplesã«ç‰¹åŒ–ã—ãŸè¨“ç·´ã«ã‚ˆã‚Šã€é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã®æ€§èƒ½ãŒå¤§å¹…å‘ä¸Š
- ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã®æ€§èƒ½ãŒã‚ãšã‹ã«ä½ä¸‹
- å…¨ä½“ã¨ã—ã¦PPLã¯æ”¹å–„

### 3. **Val PPLåŸºæº–Early Stoppingã®é‡è¦æ€§**

**æ—§æ–¹å¼ï¼ˆVal AccåŸºæº–ï¼‰**: å¤±æ•—
- æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’æ­£ã—ãé¸æŠã§ããªã„
- éå­¦ç¿’ã‚’æ¤œå‡ºã§ããªã„

**æ–°æ–¹å¼ï¼ˆVal PPLåŸºæº–ï¼‰**: æˆåŠŸ âœ…
- Epoch 8ã§æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’æ­£ã—ãé¸æŠ
- Hard PPLã‚‚åŒæ™‚ã«æœ€è‰¯ï¼ˆ571.10ï¼‰
- éå­¦ç¿’ã‚’é©åˆ‡ã«é˜²æ­¢

**çµè«–**: **Val PPLåŸºæº–ãŒå¿…é ˆ**

### 4. **Two-Stage Inferenceã®åŠ¹ç‡æ€§**

```
Shallow (Layer 2): 72.0% of samples
Deep (Layer 4):    28.0% of samples
Compute cost:      63.98%
```

**åŠ¹ç‡çš„ãªæ¨è«–**:
- 72%ã®ã‚µãƒ³ãƒ—ãƒ«ã¯Layer 2ã§çµ‚äº†ï¼ˆé«˜é€Ÿï¼‰
- 28%ã®é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã®ã¿Layer 4ä½¿ç”¨
- **36%ã®è¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›**

**Confidence Threshold**: 0.1648
- è‡ªå‹•èª¿æ•´ã«ã‚ˆã‚Šæœ€é©ãªå€¤ã‚’è¨­å®š
- è¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã§ä¸€è²«æ€§ã‚’ä¿è¨¼

### 5. **Phase 2è¨“ç·´ã®åæŸéç¨‹**

**Hard PPLã®æ¨ç§»**:
```
Epoch 1: 1711.29 â†’ Epoch 8: 571.10
æ¸›å°‘: 67%
```

**è¦³å¯Ÿ**:
- Epoch 1-8: ç¶™ç¶šçš„ãªæ”¹å–„
- Epoch 9-11: æ”¹å–„åœæ­¢ï¼ˆEarly Stoppingç™ºå‹•ï¼‰

**å­¦ç¿’ç‡**: 1e-4ï¼ˆPhase 1ã®0.1å€ï¼‰
- é©åˆ‡ãªè¨­å®šã«ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’
- éå­¦ç¿’ã‚’é˜²æ­¢

---

## ğŸ’¡ é‡è¦ãªç™ºè¦‹

### ç™ºè¦‹1: Hard Example Miningã¯éå¸¸ã«åŠ¹æœçš„

```
Hard PPLæ”¹å–„: +78.0%
```

**çµè«–**: Hard examplesã«ç‰¹åŒ–ã—ãŸè¨“ç·´ã¯ã€é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã®æ€§èƒ½ã‚’åŠ‡çš„ã«æ”¹å–„ã™ã‚‹ã€‚

### ç™ºè¦‹2: Val PPLåŸºæº–ã®Early StoppingãŒå¿…é ˆ

**Val AccåŸºæº–**: âŒ å¤±æ•—
**Val PPLåŸºæº–**: âœ… æˆåŠŸ

**ç†ç”±**: PPLã¯é€£ç¶šå€¤ã§å¾®ç´°ãªå¤‰åŒ–ã‚’æ¤œå‡ºã§ãã‚‹ã€‚Accã¯é›¢æ•£å€¤ã§ç²—ã„ã€‚

### ç™ºè¦‹3: Two-Stage Inferenceã¯åŠ¹ç‡çš„

```
Compute cost: 63.98%ï¼ˆ36%å‰Šæ¸›ï¼‰
```

**çµè«–**: ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§å¹…å‰Šæ¸›ã§ãã‚‹ã€‚

### ç™ºè¦‹4: Confidence Thresholdã®è‡ªå‹•èª¿æ•´ãŒé‡è¦

**Fixed threshold (0.8)**: âŒ 99%åé›†ï¼ˆå¤±æ•—ï¼‰
**Auto-adjusted (0.1648)**: âœ… 50%åé›†ï¼ˆæˆåŠŸï¼‰

**æ–¹æ³•**: Quantile-based threshold computation
```python
threshold = torch.quantile(all_confidences, target_ratio).item()
```

---

## ğŸ¯ çµè«–

### ä¸»è¦çµè«–

1. **Hard Example Miningã®æˆåŠŸ** âœ…
   - Hard PPL: 2599.93 â†’ 571.10 (+78.0%æ”¹å–„)
   - é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦4.5å€ã®æ€§èƒ½å‘ä¸Š

2. **Two-Stage Inferenceã®åŠ¹ç‡æ€§** âœ…
   - è¨ˆç®—ã‚³ã‚¹ãƒˆ: 63.98%ï¼ˆ36%å‰Šæ¸›ï¼‰
   - PPLæ”¹å–„: 975.07 â†’ 823.89 (-15.5%)

3. **Val PPLåŸºæº–Early Stoppingã®é‡è¦æ€§** âœ…
   - Val AccåŸºæº–ã§ã¯å¤±æ•—
   - Val PPLåŸºæº–ã§æˆåŠŸ

4. **è‡ªå‹•Thresholdèª¿æ•´ã®æœ‰åŠ¹æ€§** âœ…
   - æ­£ç¢ºã«50%ã®Hard examplesã‚’åé›†
   - Quantile-basedæ–¹å¼ãŒæœ€é©

### ä»®èª¬ã®æ¤œè¨¼çµæœ

| ä»®èª¬ | çµæœ | è¨¼æ‹  |
|------|------|------|
| Hard example miningã¯æœ‰åŠ¹ | âœ… **æˆç«‹** | Hard PPL +78.0%æ”¹å–„ |
| Two-stage inferenceã¯åŠ¹ç‡çš„ | âœ… **æˆç«‹** | Compute cost 36%å‰Šæ¸› |
| Val PPLåŸºæº–ãŒé©åˆ‡ | âœ… **æˆç«‹** | Best modelã‚’æ­£ã—ãé¸æŠ |
| è‡ªå‹•thresholdèª¿æ•´ãŒå¿…è¦ | âœ… **æˆç«‹** | æ­£ç¢ºã«50%åé›† |

---

## ğŸš€ ä»Šå¾Œã®å®Ÿé¨“ææ¡ˆ

### ææ¡ˆ1: Phase 2ã§ã‚‚å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨

**ç¾åœ¨**: Hard examplesã®ã¿ã§è¨“ç·´
**ææ¡ˆ**: å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ï¼ˆHard examplesã‚’é‡ç‚¹çš„ã«ï¼‰

**æœŸå¾…**: å…¨ä½“ã®Accuracyã‚‚æ”¹å–„

### ææ¡ˆ2: ã‚ˆã‚Šæ·±ã„ãƒ¢ãƒ‡ãƒ«

```
ç¾åœ¨: 2å±¤ â†’ 4å±¤ï¼ˆ+2å±¤ï¼‰
ææ¡ˆ: 2å±¤ â†’ 6å±¤ï¼ˆ+4å±¤ï¼‰
```

**æœŸå¾…**: Hard examplesã®æ€§èƒ½ãŒã•ã‚‰ã«æ”¹å–„

### ææ¡ˆ3: ç•°ãªã‚‹Hard exampleæ¯”ç‡

```
ç¾åœ¨: 50%
ææ¡ˆ: 30%, 70%
```

**æœŸå¾…**: æœ€é©ãªæ¯”ç‡ã‚’ç™ºè¦‹

### ææ¡ˆ4: Deep Supervision with Hard Examples

```
Phase 2ã§Deep Supervisionï¼ˆå…¨å±¤ã§å­¦ç¿’ï¼‰
```

**æœŸå¾…**: ã‚ˆã‚ŠåŠ¹æœçš„ãªè¨“ç·´

### ææ¡ˆ5: ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«

```
ç¾åœ¨: dim=64, layers=4
ææ¡ˆ: dim=128, layers=6
```

**æœŸå¾…**: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®æ¤œè¨¼

---

## ğŸ“š å‚è€ƒæƒ…å ±

### å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```python
# Phase 1
phase1_layers: 2
phase1_samples: 10000
phase1_batch: 64
phase1_epochs: 50
phase1_patience: 1
base_lr: 1e-3

# Threshold
hard_example_ratio: 0.5  # Target 50%

# Phase 2
phase2_layers: 4
phase2_batch: 64
phase2_epochs: 50
phase2_patience: 3  # Higher for new layers
phase2_lr: 1e-4  # base_lr Ã— 0.1
```

### ãƒ¢ãƒ‡ãƒ«æ§‹æˆ

```
vocab_size: 69830 (WikiText-2)
seq_len: 32
dim: 64
num_heads: 4

Phase 1: 2 layers
Phase 2: 4 layers (2 + 2)
```

### è¨“ç·´è¨­å®š

```
Phase 2å‡çµ:
- Embedding: å‡çµ
- Layer 1-2: å‡çµ

Phase 2è¨“ç·´å¯èƒ½:
- Layer 3-4: è¨“ç·´
- Output Head: è¨“ç·´

Trainable params: 50.0% (4,600,448 / 9,200,896)
```

---

## ğŸ”¬ æŠ€è¡“çš„è©³ç´°

### Confidenceè¨ˆç®—

```python
def compute_confidence(model, hidden_state):
    logits = model.output_head(hidden_state)
    probs = F.softmax(logits, dim=-1)
    return probs.max(dim=-1).values
```

### Thresholdè‡ªå‹•èª¿æ•´

```python
def compute_confidence_threshold(model, val_batches, target_ratio, device):
    all_confidences = []
    for x, y in val_batches:
        h = model.embedding(x)
        for layer in model.layers:
            h = layer(h)
        confidence = compute_confidence(model, h)
        all_confidences.append(confidence.view(-1))

    all_confidences = torch.cat(all_confidences)
    threshold = torch.quantile(all_confidences, target_ratio).item()
    return threshold
```

### Hard Examplesè©•ä¾¡

```python
def evaluate_on_hard_examples(model, hard_examples, vocab_size, device):
    hidden_states = hard_examples['hidden_states']
    targets = hard_examples['targets']

    for i in range(0, num_samples, batch_size):
        h_batch = hidden_states[i:i + batch_size].unsqueeze(1).to(device)
        y_batch = targets[i:i + batch_size].to(device)

        # Process through upper layers (if 4-layer model)
        if model.num_layers > num_lower_layers:
            for layer_idx in range(num_lower_layers, model.num_layers):
                h_batch = model.layers[layer_idx](h_batch)

        logits = model.output_head(h_batch).squeeze(1)
        loss = F.cross_entropy(logits, y_batch, reduction='sum')
        total_loss += loss.item()

    ppl = torch.exp(torch.tensor(total_loss / total_samples)).item()
    return ppl
```

### Two-Stage Inference (EASE Early Exit)

```python
# Phase 2ã§è‡ªå‹•çš„ã«DeepSupervisionTransformerã‚’ä½¿ç”¨
model_extended = DeepSupervisionTransformer(
    vocab_size=CONFIG.vocab_size,
    dim=CONFIG.dim,
    num_layers=CONFIG.phase2_layers,
    num_heads=CONFIG.num_heads,
    exit_layer=CONFIG.phase1_layers,  # Layer 2ã§Early Exit
    routing_threshold=confidence_threshold  # è‡ªå‹•è¨ˆç®—
).to(device)

# EASE frameworkã®è©•ä¾¡
eval_config = TrainingConfig(
    layer_weights={i: 0 for i in range(1, CONFIG.phase2_layers + 1)},
    routing_threshold=confidence_threshold,
    exit_layer=CONFIG.phase1_layers
)
eval_config.layer_weights[CONFIG.phase2_layers] = 1.0

eval_trainer = Trainer(eval_config, vocab_size=CONFIG.vocab_size, device=device)
stats = eval_trainer.evaluate(model_extended, val_loader)
```

---

## ã¾ã¨ã‚

**Hard Example Mining + Two-Stage Inferenceã®æ¤œè¨¼çµæœ**:

âœ… **å®Ÿé¨“æˆåŠŸ**: Hard example miningã¨Two-stage inferenceã®ä¸¡æ–¹ãŒæœ‰åŠ¹
âœ… **Hard PPLæ”¹å–„**: +78.0%ï¼ˆ2599.93 â†’ 571.10ï¼‰
âœ… **è¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›**: 36%ï¼ˆ100% â†’ 63.98%ï¼‰
âœ… **è‡ªå‹•åŒ–æˆåŠŸ**: Thresholdè‡ªå‹•èª¿æ•´ã€Val PPLåŸºæº–Early Stopping

**æ¨å¥¨**: ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å†æ¤œè¨¼ã—ã€å®Ÿç”¨æ€§ã‚’ç¢ºèª
