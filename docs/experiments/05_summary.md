# Summary of All Results

**âš ï¸ HISTORICAL DOCUMENT**: This document describes experiments conducted with the older framework that included `layer_lr_scales` as a core option. The current LEGO framework (v0.2.0) uses only **2 core options** (`layer_weights` and `routing_threshold`). See [CLAUDE.md](../../CLAUDE.md) for the latest framework specification.

---

## Terminology Mapping

| This Project (æ—§) | Academic Term | Reference |
|-------------------|---------------|-----------|
| LPT | **Deep Supervision** | Lee et al., 2015 |
| Standard Routing | **Auxiliary Loss Training** | Elbayad et al., 2020 |
| Confidence-based Routing | **Early Exit** | Teerapittayanon et al., 2016 |
| Layer-wise Learning Rate | **Discriminative Fine-Tuning** | Howard & Ruder, 2018 |

è©³ç´°ã¯ [REFERENCES.md](../REFERENCES.md) ã‚’å‚ç…§ã€‚

---

## Final Ranking

| Rank | Model | PPL | Compute% | vs Standard 3L |
|------|-------|-----|----------|----------------|
| ğŸ¥‡ | **Discriminative Fine-Tuning (Decreasing LR)** | **18.52** | 65.2% | **46.9% æ”¹å–„** |
| ğŸ¥ˆ | Discriminative Fine-Tuning (Increasing LR) | 21.14 | 72.1% | 39.3% æ”¹å–„ |
| ğŸ¥‰ | Asymmetric Auxiliary Loss (Î±=0.8) | 22.40 | 65.2% | 35.7% æ”¹å–„ |
| 4 | Asymmetric Auxiliary Loss (Î±=0.7) | 22.95 | 65.0% | 34.2% æ”¹å–„ |
| 5 | Auxiliary Loss Training (Î±=0.5) | 23.98 | 65.2% | 31.2% æ”¹å–„ |
| 6 | Deep Supervision + Early Exit | 28.13 | 46.6% | 19.3% æ”¹å–„ |
| 7 | Deep Supervision (3L) | 30.54 | 100% | 12.4% æ”¹å–„ |
| 8 | Standard (3L) | 34.86 | 100% | (baseline) |

---

## Best Practices

### 1. Training Method

| Use Case | Recommended Method | Reference |
|----------|-------------------|-----------|
| Standard Transformer | **Deep Supervision** | Lee et al., 2015 |
| **Early Exit (Best Quality)** | **Asymmetric Auxiliary Loss (Î±=0.7) + Discriminative Fine-Tuning** â­ | Ours |
| Early Exit (Alternative) | Auxiliary Loss Training (Î±=0.5) | Elbayad et al., 2020 |

### 2. Architecture Choice

| Goal | Recommended | PPL | Compute |
|------|-------------|-----|---------|
| **Best quality** | **Discriminative Fine-Tuning (Decreasing)** â­ | **18.52** | 65.2% |
| Second best | Asymmetric Auxiliary Loss (Î±=0.7) | 22.95 | 65.0% |
| Best efficiency | Deep Supervision + Early Exit | 28.13 | 46.6% |
| Simple & good | Deep Supervision (3L) | 30.54 | 100% |
| Memory constraints | Standard (1L) | 35.29 | 33.3% |

---

## Key Numbers

| Metric | Value |
|--------|-------|
| **Discriminative Fine-Tuning vs Standard 3L** | **46.9% æ”¹å–„** â­ |
| Asymmetric Auxiliary Loss (Î±=0.7) vs Standard 3L | 34.2% æ”¹å–„, 35.0% è¨ˆç®—å‰Šæ¸› |
| Auxiliary Loss Training vs Standard 3L | 31.2% æ”¹å–„, 34.8% è¨ˆç®—å‰Šæ¸› |
| Deep Supervision + Early Exit vs Standard 3L | 19.3% æ”¹å–„, 53.4% è¨ˆç®—å‰Šæ¸› |
| Deep Supervision vs Standard | 12.4% æ”¹å–„ |
| **L2ãƒ­ã‚¹è¿½åŠ ã®å½±éŸ¿** | **39.8% æ‚ªåŒ– (22.95 â†’ 32.07)** âš ï¸ |

---

## Key Insights

### 1. Deep Supervision vs Standard (for basic transformer)
- Deep Supervision ã¯12.4%æ”¹å–„
- å„å±¤ã«å‡ºåŠ›èƒ½åŠ›ã‚’æŒãŸã›ã‚‹ã“ã¨ã§æ·±ã„å±¤ã‚‚åŠ¹æœçš„ã«å­¦ç¿’

### 2. Early Exit (for efficiency)
- 31.2%æ”¹å–„ + 34.8%è¨ˆç®—å‰Šæ¸›
- ç°¡å˜ãªãƒˆãƒ¼ã‚¯ãƒ³ã¯L1ã€é›£ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã¯L3ã§å‡¦ç†

### 3. Asymmetric Auxiliary Loss (Î±=0.7, L2ãƒ­ã‚¹ãªã—)
- 34.2%æ”¹å–„
- Shallow (L1) ã‚’é‡ç‚¹çš„ã«è¨“ç·´ã™ã‚‹ã“ã¨ã§é«˜æ€§èƒ½
- å¤šãã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€Œç°¡å˜ã€ãªã®ã§L1ã®ç²¾åº¦å‘ä¸ŠãŒåŠ¹æœçš„

### 4. Discriminative Fine-Tuning (best overall) â­
- **46.9%æ”¹å–„ï¼ˆæœ€è‰¯çµæœï¼‰**
- æµ…ã„å±¤ã«é«˜ã„å­¦ç¿’ç‡ã€æ·±ã„å±¤ã«ä½ã„å­¦ç¿’ç‡
- ULMFiT (Howard & Ruder, 2018) ã§ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã‚’Early Exitã«é©ç”¨

### 5. L2ãƒ­ã‚¹ã®å½±éŸ¿ (é‡è¦ç™ºè¦‹)
- **L2ã«ãƒ­ã‚¹ã‚’è¿½åŠ ã™ã‚‹ã¨39.8%æ€§èƒ½æ‚ªåŒ–**
- L2ãŒã€Œæœ€çµ‚å‡ºåŠ›ã‚’ä½œã‚‹ã€ã‚ˆã†ã«å­¦ç¿’ã—ã¦ã—ã¾ã†
- L2ã¯ç´”ç²‹ãªä¸­é–“å±¤ã¨ã—ã¦æ©Ÿèƒ½ã•ã›ã‚‹ã¹ã
- L2ãƒ­ã‚¹ãªã—ã®å ´åˆã€L2ã¯Deep pathã®ç‰¹å¾´æŠ½å‡ºã«å°‚å¿µ

### 6. æ•°å­¦çš„åŒç­‰æ€§
- L2ã«ãƒ­ã‚¹ã‚’é©ç”¨ã—ãªã„å ´åˆã€ä»¥ä¸‹ã¯åŒç­‰:
  - `forward_all_layers()` ã§L1, L3ã®ã¿ãƒ­ã‚¹
  - `forward_train()` ã§shallow, deepã®ã¿ãƒ­ã‚¹

---

## Recommended Configuration (Universal Framework)

```python
from experiments.universal_trainer import UniversalConfig, PRESETS

# For best quality (æ¨å¥¨) â­
config = UniversalConfig(
    layer_weights={1: 0.7, 2: 0, 3: 0.3},
    routing_threshold=0.95,
    layer_lr_scales={1: 1.0, 2: 0.5, 3: 0.1}  # Discriminative Fine-Tuning
)

# Alternative (simpler to understand)
config = PRESETS['auxiliary_loss']
# Equivalent to:
# UniversalConfig(layer_weights={1: 0.5, 2: 0, 3: 0.5}, routing_threshold=0.95)

# For best efficiency (speed-focused)
config = PRESETS['deep_supervision_routing']
# Equivalent to:
# UniversalConfig(layer_weights={1: 1/3, 2: 1/3, 3: 1/3}, routing_threshold=0.7)

# For simplicity (no routing)
config = PRESETS['deep_supervision']
# Equivalent to:
# UniversalConfig(layer_weights={1: 1/3, 2: 1/3, 3: 1/3}, routing_threshold=0)
```

è©³ç´°ã¯ [06_universal_framework.md](06_universal_framework.md) ã‚’å‚ç…§ã€‚

---

## Experimental Notes

### Early Stopping
All models converge in **1 epoch** with strict early stopping due to:
1. Quick learning of basic patterns
2. Overfitting on small validation set
3. Early stopping preserves generalization

### Validation Behavior
- Train PPL continues to decrease
- Val PPL increases after epoch 1
- This indicates overfitting, not underfitting

---

## Future Work

1. **Larger models**: Test if findings scale
2. **More data**: Reduce overfitting tendency
3. **Different tasks**: Verify routing helps across tasks
4. **Î± ã®æœ€é©åŒ–**: ã‚ˆã‚Šç´°ã‹ã„ Î± å€¤ã®æ¢ç´¢ (0.6, 0.75, 0.8 ãªã©)
5. **Multi-exit**: è¤‡æ•°ã® exit point ã‚’æŒã¤ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
6. **å­¦ç¿’å¯èƒ½ãªConfidence Head**: max(softmax)ä»¥å¤–ã®æ‰‹æ³•

## References

- Lee, C.-Y., et al. (2015). **Deeply-Supervised Nets**. AISTATS 2015. https://arxiv.org/abs/1409.5185
- Howard, J., & Ruder, S. (2018). **Universal Language Model Fine-tuning for Text Classification**. ACL 2018. https://arxiv.org/abs/1801.06146
- Elbayad, M., et al. (2020). **Depth-Adaptive Transformer**. ICLR 2020. https://arxiv.org/abs/1910.10073
- Teerapittayanon, S., et al. (2016). **BranchyNet: Fast Inference via Early Exiting**. ICPR 2016. https://arxiv.org/abs/1709.01686
