# Progressive Layer Training Experiment

**å®Ÿé¨“æ—¥**: 2025-12-12
**å®Ÿé¨“ã‚³ãƒ¼ãƒ‰**: colab1.py
**ãƒ‡ãƒã‚¤ã‚¹**: NVIDIA L4 GPU (Google Colab)

---

## ğŸ“‹ å®Ÿé¨“æ¦‚è¦

### ç›®çš„

Deep SupervisionãŒæ®µéšçš„ãªå±¤è¿½åŠ è¨“ç·´ï¼ˆProgressive Layer Trainingï¼‰ã«æœ‰åˆ©ã‹ã‚’æ¤œè¨¼ã™ã‚‹ã€‚

### ä»®èª¬

- **Standard Transformer**: æœ€çµ‚å±¤ã®ã¿ã§å­¦ç¿’ â†’ æ–°å±¤è¿½åŠ æ™‚ã«æ—¢å­˜å±¤ã¨ã®é€£æºãŒé›£ã—ã„
- **Deep Supervision**: å…¨å±¤ã§ç‹¬ç«‹å­¦ç¿’ â†’ æ—¢å­˜å±¤ãŒå®‰å®š â†’ æ–°å±¤è¿½åŠ ãŒåŠ¹æœçš„

### å®Ÿé¨“è¨­è¨ˆ

**Phase 1: åˆæœŸè¨“ç·´ï¼ˆ3å±¤ï¼‰**
- 3å±¤ãƒ¢ãƒ‡ãƒ«ã‚’é€šå¸¸é€šã‚Šè¨“ç·´
- ãƒ‡ãƒ¼ã‚¿: WikiText-2 (10K samples)
- Patience: 1

**Phase 2: å±¤è¿½åŠ  + é¸æŠçš„è¨“ç·´ï¼ˆ4å±¤ï¼‰**
- 1å±¤ã‚’è¿½åŠ ï¼ˆãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ï¼‰
- æ—¢å­˜3å±¤ã‚’å‡çµ
- æ–°ã—ã„Layer 4ã®ã¿è¨“ç·´
- å­¦ç¿’ç‡: 1e-4ï¼ˆbase_lr Ã— 0.1ï¼‰
- Patience: 1

---

## ğŸ“Š å®Ÿé¨“çµæœ

### Standard Transformer

| Phase | Layers | Best Acc | Best PPL | Time | Best Epoch |
|-------|--------|----------|----------|------|------------|
| Phase 1 | 3å±¤ | **16.35%** | 978.89 | 23.69s | 3 |
| Phase 2 | 4å±¤ | **16.20%** | 975.51 | 27.44s | 2 |
| **å¤‰åŒ–** | +1å±¤ | **-0.15%** | **+0.34%** | +3.75s | - |

### Deep Supervision Transformer

| Phase | Layers | Best Acc | Best PPL | Time | Best Epoch |
|-------|--------|----------|----------|------|------------|
| Phase 1 | 3å±¤ | **16.45%** | 950.45 | 53.06s | 3 |
| Phase 2 | 4å±¤ | **16.26%** | 968.60 | 92.11s | 3 |
| **å¤‰åŒ–** | +1å±¤ | **-0.19%** | **-1.91%** | +39.05s | - |

### æ¯”è¼ƒ

| ãƒ¢ãƒ‡ãƒ« | Accå¤‰åŒ– | PPLå¤‰åŒ– | åˆè¨ˆæ™‚é–“ |
|--------|---------|---------|----------|
| Standard | -0.15% | +0.34% | 51.13s |
| Deep Supervision | -0.19% | -1.91% | 145.16s |

---

## ğŸ” è©³ç´°åˆ†æ

### 1. **ä¸¡ãƒ¢ãƒ‡ãƒ«ã¨ã‚‚ç²¾åº¦ãŒä½ä¸‹ã—ãŸ**

```
Standard:        16.35% â†’ 16.20% (-0.15%)
Deep Supervision: 16.45% â†’ 16.26% (-0.19%)
```

**åŸå› **:
- æ–°ã—ã„Layer 4ãŒãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–
- ã‚ãšã‹2-3ã‚¨ãƒãƒƒã‚¯ã§è¨“ç·´çµ‚äº†ï¼ˆEarly Stoppingï¼‰
- æ–°å±¤ãŒååˆ†ã«å­¦ç¿’ã§ãã¦ã„ãªã„

---

### 2. **PPLã®å¤‰åŒ–ãŒå¯¾ç…§çš„**

```
Standard:        978.89 â†’ 975.51 (+0.34% æ”¹å–„)
Deep Supervision: 950.45 â†’ 968.60 (-1.91% æ‚ªåŒ–)
```

**Standard**: ã‚ãšã‹ã«PPLæ”¹å–„
**Deep Supervision**: PPLæ‚ªåŒ–

**è§£é‡ˆ**:
- Deep Supervisionã¯æ—¢å­˜å±¤ãŒå›ºå®šçš„ã™ãã‚‹å¯èƒ½æ€§
- æ–°å±¤è¿½åŠ ã«ã‚ˆã‚Šå…¨ä½“ã®ãƒãƒ©ãƒ³ã‚¹ãŒå´©ã‚ŒãŸ
- Standardã®æ–¹ãŒæŸ”è»Ÿã«é©å¿œ

---

### 3. **è¨“ç·´æ™‚é–“ã®å¤§ããªå·®**

```
Standard:        51.13ç§’ï¼ˆåˆè¨ˆï¼‰
Deep Supervision: 145.16ç§’ï¼ˆåˆè¨ˆï¼‰ â† 2.8å€
```

**Phase 2ã§ã®å·®ãŒé¡•è‘—**:
- Standard: 27.44ç§’
- Deep Supervision: 92.11ç§’ï¼ˆ3.4å€ï¼‰

**åŸå› **: Deep Supervisionã¯å…¨å±¤ã§forward_all_layers()ã‚’ä½¿ç”¨

---

### 4. **å­¦ç¿’ç‡å‰Šæ¸›ã®åŠ¹æœ**

Phase 2ã§å­¦ç¿’ç‡ã‚’1e-4ã«å‰Šæ¸›ï¼ˆbase_lr Ã— 0.1ï¼‰ã—ãŸåŠ¹æœ:

**Standard**:
```
Epoch 1: Val PPL 1003.76
Epoch 2: Val PPL 975.51  â† Best
Epoch 3: Val PPL 983.27
```
â†’ Epoch 2ã§æœ€è‰¯ã€å®‰å®šã—ãŸå­¦ç¿’

**Deep Supervision**:
```
Epoch 1: Val PPL 1035.27
Epoch 2: Val PPL 977.14
Epoch 3: Val PPL 968.60  â† Best
Epoch 4: Val PPL 974.80
```
â†’ ã‚ˆã‚Šé•·ãè¨“ç·´ï¼ˆ3ã‚¨ãƒãƒƒã‚¯ï¼‰ã€å¾ã€…ã«æ”¹å–„

**çµè«–**: å­¦ç¿’ç‡å‰Šæ¸›ã¯é©åˆ‡ã ã£ãŸ

---

### 5. **Early Stoppingã®å½±éŸ¿**

Patience=1ã®å½±éŸ¿:

**Standard Phase 2**:
- Best: Epoch 2
- åœæ­¢: Epoch 5ï¼ˆ3ã‚¨ãƒãƒƒã‚¯é€£ç¶šæ‚ªåŒ–ï¼‰

**Deep Supervision Phase 2**:
- Best: Epoch 3
- åœæ­¢: Epoch 6ï¼ˆ3ã‚¨ãƒãƒƒã‚¯é€£ç¶šæ‚ªåŒ–ï¼‰

**æ³¨æ„**: patience=1ã ãŒã€å®Ÿéš›ã¯3ã‚¨ãƒãƒƒã‚¯é€£ç¶šæ‚ªåŒ–ã§åœæ­¢ã—ã¦ã„ã‚‹
â†’ ã‚³ãƒ¼ãƒ‰ã«ä¸æ•´åˆãŒã‚ã‚‹å¯èƒ½æ€§ï¼ˆè¦èª¿æŸ»ï¼‰

---

## ğŸ’¡ é‡è¦ãªç™ºè¦‹

### ç™ºè¦‹1: 1å±¤è¿½åŠ ã§ã¯åŠ¹æœãŒé™å®šçš„

```
ç²¾åº¦å¤‰åŒ–: -0.15% ~ -0.19%ï¼ˆã‚€ã—ã‚æ‚ªåŒ–ï¼‰
```

**ç†ç”±**:
- 3å±¤ â†’ 4å±¤ï¼ˆ+33%å¢—åŠ ï¼‰ã ãŒã€æ–°å±¤ã®ã¿è¨“ç·´
- æ—¢å­˜å±¤ãŒå‡çµ â†’ æ–°å±¤ã¨ã®é€£æºãŒä¸ååˆ†
- ã‚ãšã‹2-3ã‚¨ãƒãƒƒã‚¯ â†’ å­¦ç¿’ä¸è¶³

### ç™ºè¦‹2: Deep Supervisionã®å„ªä½æ€§ãªã—

ä»®èª¬: Deep Supervisionã¯æ®µéšçš„è¨“ç·´ã«æœ‰åˆ©
**çµæœ**: **ä»®èª¬ã¯ä¸æˆç«‹**

```
Standard:        Acc -0.15%, PPL +0.34%ï¼ˆã‚ãšã‹ã«æ”¹å–„ï¼‰
Deep Supervision: Acc -0.19%, PPL -1.91%ï¼ˆæ‚ªåŒ–ï¼‰
```

â†’ ã‚€ã—ã‚Standardã®æ–¹ãŒè‰¯å¥½

### ç™ºè¦‹3: è¨“ç·´æ™‚é–“ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

```
Deep Supervision: 2.8å€é…ã„ï¼ˆ145.16s vs 51.13sï¼‰
ç²¾åº¦å‘ä¸Š: ãªã—ï¼ˆã‚€ã—ã‚æ‚ªåŒ–ï¼‰
```

â†’ ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒéå¸¸ã«æ‚ªã„

---

## ğŸ¯ çµè«–

### ä¸»è¦çµè«–

1. **1å±¤è¿½åŠ ã¯åŠ¹æœãŒé™å®šçš„**
   - ç²¾åº¦: -0.15% ~ -0.19%ï¼ˆæ‚ªåŒ–ï¼‰
   - æ–°å±¤ãŒååˆ†å­¦ç¿’ã§ããªã„

2. **Deep Supervisionã®å„ªä½æ€§ãªã—**
   - æ®µéšçš„è¨“ç·´ã§ã®åˆ©ç‚¹ã¯ç¢ºèªã§ããš
   - ã‚€ã—ã‚Standardã®æ–¹ãŒè‰¯å¥½

3. **è¨“ç·´æ™‚é–“ã®ã‚³ã‚¹ãƒˆé«˜**
   - Deep Supervision: 2.8å€é…ã„
   - ç²¾åº¦å‘ä¸Šãªã— â†’ å‰²ã«åˆã‚ãªã„

### ä»®èª¬ã®æ¤œè¨¼çµæœ

| ä»®èª¬ | çµæœ | ç†ç”± |
|------|------|------|
| Deep Supervisionã¯æ–°å±¤è¿½åŠ ã«æœ‰åˆ© | âŒ **ä¸æˆç«‹** | ç²¾åº¦æ‚ªåŒ–ã€PPLæ‚ªåŒ– |
| æ—¢å­˜å±¤ã®å®‰å®šæ€§ãŒé‡è¦ | âš ï¸ **éƒ¨åˆ†çš„** | å‡çµã«ã‚ˆã‚Šé€£æºä¸è¶³ |
| æ®µéšçš„è¨“ç·´ã¯åŠ¹æœçš„ | âŒ **ä¸æˆç«‹** | 1å±¤ã§ã¯åŠ¹æœãªã— |

---

## ğŸš€ ä»Šå¾Œã®å®Ÿé¨“ææ¡ˆ

### ææ¡ˆ1: ã‚ˆã‚Šå¤šãã®å±¤ã‚’è¿½åŠ 

```
ç¾åœ¨: 3å±¤ â†’ 4å±¤ï¼ˆ+1å±¤ï¼‰
ææ¡ˆ: 3å±¤ â†’ 6å±¤ï¼ˆ+3å±¤ï¼‰
```

**æœŸå¾…**: ã‚ˆã‚Šæ˜ç¢ºãªå·®ãŒå‡ºã‚‹å¯èƒ½æ€§

### ææ¡ˆ2: ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«

```
ç¾åœ¨: dim=64, layers=3
ææ¡ˆ: dim=128, layers=6
```

**æœŸå¾…**: Deep Supervisionã®åŠ¹æœãŒæ‹¡å¤§ã™ã‚‹å¯èƒ½æ€§

### ææ¡ˆ3: æ®µéšçš„ãªå‡çµè§£é™¤

```
Phase 1: 3å±¤è¨“ç·´
Phase 2: +1å±¤è¿½åŠ ã€Layer 1-3å‡çµ
Phase 3: Layer 3å‡çµè§£é™¤ã€Layer 1-2å‡çµ
Phase 4: å…¨å±¤è¨“ç·´å¯èƒ½
```

**æœŸå¾…**: ã‚ˆã‚ŠæŸ”è»Ÿãªé©å¿œ

### ææ¡ˆ4: ã‚ˆã‚Šé•·ã„è¨“ç·´

```
ç¾åœ¨: patience=1ï¼ˆ2-3ã‚¨ãƒãƒƒã‚¯ã§åœæ­¢ï¼‰
ææ¡ˆ: patience=5, max_epochs=100
```

**æœŸå¾…**: æ–°å±¤ãŒååˆ†å­¦ç¿’ã§ãã‚‹

### ææ¡ˆ5: ç•°ãªã‚‹å­¦ç¿’ç‡æˆ¦ç•¥

```
ç¾åœ¨: æ–°å±¤ 1e-4ï¼ˆbase_lr Ã— 0.1ï¼‰
ææ¡ˆ: æ–°å±¤ 1e-3, æ—¢å­˜å±¤ 1e-5ï¼ˆå‡çµè§£é™¤æ™‚ï¼‰
```

**æœŸå¾…**: ã‚ˆã‚ŠåŠ¹æœçš„ãªå±¤é–“é€£æº

---

## ğŸ“š å‚è€ƒæƒ…å ±

### å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```python
# Phase 1
phase1_samples: 10000
phase1_batch: 64
phase1_epochs: 50
phase1_patience: 1
base_lr: 1e-3

# Phase 2
phase2_samples: 10000
phase2_batch: 64
phase2_epochs: 50
phase2_patience: 1
reduced_lr: 1e-4  # base_lr Ã— 0.1
```

### ãƒ¢ãƒ‡ãƒ«æ§‹æˆ

```
vocab_size: 69830 (WikiText-2)
seq_len: 32
dim: 64
initial_layers: 3
added_layers: 1
num_heads: 4
```

### å‡çµè¨­å®š

```
Phase 2ã§å‡çµ:
- Embedding
- Layer 1-3

Phase 2ã§è¨“ç·´å¯èƒ½:
- Layer 4 (æ–°è¦)
- Output Head

è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 49.3% (4,534,784 / 9,200,896)
```

---

## ğŸ”¬ æŠ€è¡“çš„è©³ç´°

### å±¤è¿½åŠ ã®å®Ÿè£…

```python
def add_layer_to_model(model, device):
    # 4å±¤ãƒ¢ãƒ‡ãƒ«ã‚’æ–°è¦ä½œæˆ
    new_model = Model(num_layers=4)

    # æ—¢å­˜ã®é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼
    new_model.embedding â† model.embedding
    new_model.layers[0:3] â† model.layers[0:3]
    new_model.output_head â† model.output_head

    # Layer 4ã¯ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ï¼ˆè‡ªå‹•ï¼‰
    return new_model
```

### å‡çµã®å®Ÿè£…

```python
def freeze_initial_layers(model, num_layers_to_freeze=3):
    # Embeddingå‡çµ
    model.embedding.requires_grad = False

    # Layer 1-3å‡çµ
    for i in range(3):
        model.layers[i].requires_grad = False

    # Output Headè¨“ç·´å¯èƒ½
    model.output_head.requires_grad = True
```

---

## ã¾ã¨ã‚

**Progressive Layer Trainingï¼ˆ1å±¤è¿½åŠ ï¼‰ã®æ¤œè¨¼çµæœ**:

âœ… **å®Ÿé¨“è‡ªä½“ã¯æˆåŠŸ**: æ®µéšçš„è¨“ç·´ã®å®Ÿè£…ã¨æ¤œè¨¼ãŒã§ããŸ
âŒ **ä»®èª¬ã¯ä¸æˆç«‹**: Deep Supervisionã®å„ªä½æ€§ãªã—
âš ï¸ **åŠ¹æœã¯é™å®šçš„**: 1å±¤è¿½åŠ ã§ã¯ç²¾åº¦å‘ä¸Šãªã—

**æ¨å¥¨**: ã‚ˆã‚Šå¤§è¦æ¨¡ãªå®Ÿé¨“ï¼ˆ+3å±¤ã€dim=128ãªã©ï¼‰ã§å†æ¤œè¨¼ãŒå¿…è¦
