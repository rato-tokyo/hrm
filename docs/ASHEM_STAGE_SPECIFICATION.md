# ASHEM Stage Specification - Detailed Technical Documentation

**Version**: 1.0
**Date**: 2025-12-12
**Status**: Production-ready (Verified with commit fc9b140)

---

## ğŸ“Œ é‡è¦ãªå®šç¾©: "Stage" ã¨ã¯ä½•ã‹

### Stage ã®æ­£ç¢ºãªå®šç¾©

**Stage (ã‚¹ãƒ†ãƒ¼ã‚¸)** = **è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚ºã®æ™‚é–“çš„åŒºåˆ‡ã‚Š**

- **æ™‚é–“è»¸ä¸Šã®åŒºåˆ‡ã‚Š**: ç•°ãªã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§å®Ÿè¡Œã•ã‚Œã‚‹è¨“ç·´ã®æ®µéš
- **ç‹¬ç«‹ã—ãŸè¨“ç·´ãƒ«ãƒ¼ãƒ—**: å„Stageã¯ç‹¬è‡ªã®ãƒ‡ãƒ¼ã‚¿ã€ãƒ¢ãƒ‡ãƒ«æ§‹æˆã€Early Stoppingã‚’æŒã¤
- **ç´¯ç©çš„ãªãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰**: å‰ã®Stageã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¬¡ã®Stageã§æ‹¡å¼µãƒ»æ”¹è‰¯

### ASHEM ã«ãŠã‘ã‚‹ Stage ã®å®Ÿè£…

ASHEM ã¯ **2-Stage Training** ã‚’æ¡ç”¨ï¼š

```
Stage 1 (Phase 1) â†’ Stage 2 (Phase 2)
     â†“                    â†“
æ™‚åˆ» t=0ï½tâ‚         æ™‚åˆ» tâ‚ï½tâ‚‚
```

**é‡è¦**: "Stage" ã¨ "Phase" ã¯æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯åŒç¾©èªã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚

---

## ğŸ” Stage ã®è©³ç´°ä»•æ§˜

### Stage 1 (Phase 1): Shallow Model Training

#### ç›®çš„
å…¨ãƒ‡ãƒ¼ã‚¿ã§æµ…å±¤ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€Hard Examples ã‚’è­˜åˆ¥ã™ã‚‹åŸºæº–ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰

#### ãƒ¢ãƒ‡ãƒ«æ§‹æˆ
- **å±¤æ•°**: 2å±¤ (phase1_layers=2)
- **ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹**: `StandardTransformer` ã¾ãŸã¯ `DeepSupervisionTransformer`
- **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: å…¨ãƒ‡ãƒ¼ã‚¿ (WikiText-2 10K samples)

#### è¨“ç·´è¨­å®š
```python
# TrainingConfig for Stage 1
config = TrainingConfig(
    layer_weights={1: 0, 2: 1}  # æœ€çµ‚å±¤ã®ã¿ã§æå¤±è¨ˆç®—
)

# Early Stoppingè¨­å®š
patience = 1  # ASHEMConfig.phase1_patience
learning_rate = 1e-3  # ASHEMConfig.phase1_lr
```

#### è¨“ç·´ãƒ«ãƒ¼ãƒ—
```python
for epoch in range(max_epochs):
    # å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
    train_loss = trainer.train_epoch(model, train_loader, optimizer)

    # æ¤œè¨¼
    val_stats = trainer.evaluate(model, val_loader)
    val_ppl = val_stats['ppl']

    # Early Stoppingåˆ¤å®š
    if val_ppl < best_val_ppl:
        best_val_ppl = val_ppl
        save_model(model)
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= patience:
            break  # Early Stopping
```

#### Stage 1 ã®å‡ºåŠ›
1. **è¨“ç·´æ¸ˆã¿æµ…å±¤ãƒ¢ãƒ‡ãƒ«**: 2å±¤ã®Transformer
2. **Confidence Threshold**: Hard Exampleè­˜åˆ¥ã®ãŸã‚ã®é–¾å€¤
3. **Hard Examples**: æµ…å±¤ãƒ¢ãƒ‡ãƒ«ãŒè‹¦æ‰‹ã¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«é›†åˆ

#### æœŸå¾…ã•ã‚Œã‚‹çµæœ (WikiText-2, 10K samples)
```
Best Val PPL: 986.43
Best Val Acc: 16.03%
Hard PPL (2å±¤ãƒ¢ãƒ‡ãƒ«ã§ã®Hard examplesæ€§èƒ½): 2763.69
```

---

### Stage é–“ã®å‡¦ç†: Hard Example Mining

Stage 1 ã¨ Stage 2 ã®é–“ã«å®Ÿè¡Œã•ã‚Œã‚‹**é‡è¦ãªä¸­é–“å‡¦ç†**ï¼š

#### 1. Confidence Threshold è¨ˆç®—

**ç›®çš„**: Hard Examples ã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã®é–¾å€¤ã‚’æ±ºå®š

**å®Ÿè£…** (Per-token filtering):
```python
def compute_confidence_threshold(
    model: nn.Module,
    val_batches: List[Tuple[torch.Tensor, torch.Tensor]],
    target_ratio: float,  # 0.5 = 50% of tokens
    device: str
) -> float:
    """
    Per-token quantile calculation.

    å„ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«ä¿¡é ¼åº¦ã‚’è¨ˆç®—ã—ã€target_ratioåˆ†ä½ç‚¹ã‚’é–¾å€¤ã¨ã™ã‚‹ã€‚
    """
    all_confidences = []

    for x, _ in val_batches:
        x = x.to(device)

        # Forward through all layers
        h = model.embedding(x)
        for layer in model.layers:
            h = layer(h)

        # Compute per-token confidence
        logits = model.output_head(h)
        probs = F.softmax(logits, dim=-1)
        confidence = probs.max(dim=-1).values  # (batch, seq_len)

        # âš ï¸ CRITICAL: Flatten to per-token
        all_confidences.append(confidence.view(-1))

    # Compute threshold
    all_confidences = torch.cat(all_confidences)
    threshold = torch.quantile(all_confidences, target_ratio).item()

    return threshold
```

**âš ï¸ é‡è¦**: `.view(-1)` ã«ã‚ˆã‚‹Per-token flatteningãŒå¿…é ˆ

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›** (WikiText-2, 10K samples, target_ratio=0.5):
```
Threshold: 0.1499
Interpretation: ä¿¡é ¼åº¦ < 0.1499 ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ Hard ã¨ã¿ãªã™
```

#### 2. Hard Examples åé›†

**ç›®çš„**: é–¾å€¤ä»¥ä¸‹ã®Confidenceã‚’æŒã¤ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åé›†

**å®Ÿè£…** (Per-token filtering):
```python
def collect_hard_examples(
    model: nn.Module,
    val_batches: List[Tuple[torch.Tensor, torch.Tensor]],
    threshold: float,
    device: str
) -> Dict[str, torch.Tensor]:
    """
    Per-token filtering to collect hard examples.

    å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å€‹åˆ¥ã«è©•ä¾¡ã—ã€é–¾å€¤ä»¥ä¸‹ã®ã‚‚ã®ã‚’åé›†ã€‚
    """
    hard_inputs = []
    hard_hidden_states = []
    hard_targets = []
    hard_confidences = []

    for x, y in val_batches:
        x, y = x.to(device), y.to(device)

        # Forward through all layers
        h = model.embedding(x)
        for layer in model.layers:
            h = layer(h)

        # Compute per-token confidence
        logits = model.output_head(h)
        probs = F.softmax(logits, dim=-1)
        confidence = probs.max(dim=-1).values  # (batch, seq_len)

        # âš ï¸ CRITICAL: Per-token comparison
        mask = confidence < threshold  # (batch, seq_len)

        # Flatten and filter
        x_flat = x.view(-1)
        h_flat = h.view(-1, h.shape[-1])
        y_flat = y.view(-1)
        confidence_flat = confidence.view(-1)
        mask_flat = mask.view(-1)

        # Collect hard examples
        hard_inputs.append(x_flat[mask_flat])
        hard_hidden_states.append(h_flat[mask_flat])
        hard_targets.append(y_flat[mask_flat])
        hard_confidences.append(confidence_flat[mask_flat])

    return {
        'inputs': torch.cat(hard_inputs),
        'hidden_states': torch.cat(hard_hidden_states),
        'targets': torch.cat(hard_targets),
        'confidences': torch.cat(hard_confidences)
    }
```

**âš ï¸ é‡è¦**: Thresholdè¨ˆç®—ã¨åŒã˜Per-tokenæ–¹å¼ã‚’ä½¿ç”¨

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›** (WikiText-2, 10K samples):
```
Collected hard examples: 32,768 tokens
Average confidence: 0.0653
Actual ratio: 51.2% (target: 50%)
```

---

### Stage 2 (Phase 2): Deep Model Training on Hard Examples

#### ç›®çš„
Hard Examples ã«ç‰¹åŒ–ã—ã¦æ·±å±¤ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã§ã®æ€§èƒ½å‘ä¸Š

#### ãƒ¢ãƒ‡ãƒ«æ§‹æˆ
- **å±¤æ•°**: 4å±¤ (phase2_layers=4)
- **ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹**: `DeepSupervisionTransformer` (Early Exit ã‚µãƒãƒ¼ãƒˆ)
- **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: **Hard Examples ã®ã¿** (ç´„32,768 tokens)
- **åˆæœŸåŒ–**:
  - Layer 1-2: Stage 1 ã®é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼ (**Frozen**)
  - Layer 3-4: ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ– (**Trainable**)

#### Hard Freezing è¨­å®š
```python
# Freeze lower layers (Stage 1ã§è¨“ç·´æ¸ˆã¿)
for param in model_extended.embedding.parameters():
    param.requires_grad = False

for i in range(phase1_layers):  # i=0,1 (Layer 1-2)
    for param in model_extended.layers[i].parameters():
        param.requires_grad = False

# Layer 3-4 ã¯è‡ªå‹•çš„ã« trainable (requires_grad=True)
```

**é‡è¦**: Hard Freezing = `requires_grad=False` ã«ã‚ˆã‚‹å®Œå…¨ãªå‡çµ

#### è¨“ç·´è¨­å®š
```python
# TrainingConfig for Stage 2
phase2_config = TrainingConfig(
    layer_weights={1: 0, 2: 0, 3: 0, 4: 1}  # æœ€çµ‚å±¤ã®ã¿ã§æå¤±è¨ˆç®—
)

# Early Stoppingè¨­å®š
patience = 3  # ASHEMConfig.phase2_patience
learning_rate = 1e-4  # ASHEMConfig.phase2_lr (Stage 1ã‚ˆã‚Šä½ã„)
```

#### è¨“ç·´ãƒ«ãƒ¼ãƒ—
```python
for epoch in range(max_epochs):
    # Hard Examples ã®ã¿ã§è¨“ç·´
    train_loss = train_upper_layers(
        model_extended, hard_batches, optimizer_upper,
        vocab_size, device, num_lower_layers=2
    )

    # âš ï¸ CRITICAL: Early Exit ã‚’ä½¿ç”¨ã—ã¦æ¤œè¨¼
    eval_config = TrainingConfig(
        layer_weights={1: 0, 2: 0, 3: 0, 4: 1},
        routing_threshold=confidence_threshold,  # Stageé–“ã§è¨ˆç®—ã—ãŸé–¾å€¤
        exit_layer=2  # Layer 2 ã§ Early Exit å¯èƒ½
    )
    eval_trainer = Trainer(eval_config, vocab_size, device)
    val_stats = eval_trainer.evaluate(model_extended, val_loader)
    val_ppl = val_stats['ppl']

    # Hard Examples ã§ã®æ€§èƒ½è©•ä¾¡
    hard_ppl = evaluate_on_hard_examples(
        model_extended, hard_examples, vocab_size, device,
        batch_size=64, num_lower_layers=2
    )

    print(f"Epoch {epoch+1} - Val PPL: {val_ppl:.2f} | Hard PPL: {hard_ppl:.2f}")

    # Early Stoppingåˆ¤å®š (Val PPLåŸºæº–)
    if val_ppl < best_val_ppl:
        best_val_ppl = val_ppl
        save_model(model_extended)
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= patience:
            break  # Early Stopping
```

**âš ï¸ é‡è¦ãªå®Ÿè£…è©³ç´°**:
1. **è¨“ç·´**: Hard Examples ã®ã¿ä½¿ç”¨
2. **æ¤œè¨¼**: å…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ (Early Exit æœ‰åŠ¹)
3. **Early Stoppingåˆ¤å®š**: Val PPL åŸºæº– (**Hard PPL ã§ã¯ãªã„**)

#### Stage 2 ã®å‡ºåŠ›
1. **è¨“ç·´æ¸ˆã¿æ·±å±¤ãƒ¢ãƒ‡ãƒ«**: 4å±¤ã®Transformer (Layer 1-2ã¯å‡çµã€Layer 3-4ã¯è¨“ç·´æ¸ˆã¿)
2. **æ¤œè¨¼æ€§èƒ½**: Val PPL, Val Acc
3. **Hard Examplesæ€§èƒ½**: Hard PPL

#### æœŸå¾…ã•ã‚Œã‚‹çµæœ (WikiText-2, 10K samples)
```
Best Epoch: 7
Best Val PPL: 829.78 (Early Exitä½¿ç”¨æ™‚)
Hard PPL: 668.08 (4å±¤ãƒ¢ãƒ‡ãƒ«ã§ã®Hard examplesæ€§èƒ½)
Hard PPL Improvement: +2095.60 (+75.8%)
```

---

## ğŸ”„ Stageé–“ã®é‡è¦ãªé•ã„

### ãƒ‡ãƒ¼ã‚¿ã®é•ã„

| Stage | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ | ãƒ‡ãƒ¼ã‚¿é‡ | é¸æŠåŸºæº– |
|-------|-----------|---------|---------|
| Stage 1 | **å…¨ãƒ‡ãƒ¼ã‚¿** | 100% (~64,000 tokens) | ãªã— |
| Stage 2 | **Hard Examples ã®ã¿** | ç´„50% (~32,768 tokens) | Confidence < Threshold |

### ãƒ¢ãƒ‡ãƒ«ã®é•ã„

| Stage | å±¤æ•° | åˆæœŸåŒ– | è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
|-------|-----|-------|------------------|
| Stage 1 | 2å±¤ | ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ– | 100% (å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) |
| Stage 2 | 4å±¤ | Layer 1-2: ã‚³ãƒ”ãƒ¼<br>Layer 3-4: ãƒ©ãƒ³ãƒ€ãƒ  | 50% (Layer 3-4ã®ã¿) |

### è¨“ç·´è¨­å®šã®é•ã„

| Stage | Learning Rate | Patience | æœŸå¾…ã•ã‚Œã‚‹Epochæ•° |
|-------|--------------|----------|------------------|
| Stage 1 | 1e-3 (é«˜ã„) | 1 (å³ã—ã„) | 3-4 epochs |
| Stage 2 | 1e-4 (ä½ã„) | 3 (ç·©ã„) | 7-10 epochs |

### è©•ä¾¡ã®é•ã„

| Stage | è©•ä¾¡å¯¾è±¡ | Early Exit | è©•ä¾¡æŒ‡æ¨™ |
|-------|---------|-----------|---------|
| Stage 1 | å…¨ãƒ‡ãƒ¼ã‚¿ | ãªã— | Val PPL, Val Acc |
| Stage 2 | å…¨ãƒ‡ãƒ¼ã‚¿ + Hard Examples | **ã‚ã‚Š** | Val PPL, Val Acc, Hard PPL |

---

## âš ï¸ Critical Implementation Details

### 1. Per-token Filtering ã®å¿…é ˆæ€§

**çµ¶å¯¾ã«å®ˆã‚‹ã¹ããƒ«ãƒ¼ãƒ«**: Thresholdè¨ˆç®—ã¨Filteringæ–¹å¼ã‚’ä¸€è‡´ã•ã›ã‚‹

#### æ­£ã—ã„å®Ÿè£… (Per-token):
```python
# Thresholdè¨ˆç®—
confidence = compute_confidence(model, h)  # (batch, seq_len)
all_confidences.append(confidence.view(-1))  # â† Flatten per-token
threshold = torch.quantile(torch.cat(all_confidences), target_ratio)

# Filtering
mask = confidence < threshold  # (batch, seq_len) â† Per-token comparison
```

#### é–“é•ã£ãŸå®Ÿè£… (æ··åœ¨):
```python
# âŒ Thresholdè¨ˆç®—: Per-token
confidence = compute_confidence(model, h)  # (batch, seq_len)
all_confidences.append(confidence.view(-1))  # Per-token
threshold = torch.quantile(torch.cat(all_confidences), target_ratio)

# âŒ Filtering: Sequence-level
mask = confidence.mean(dim=1) < threshold  # â† é–“é•ã„ï¼æ–¹å¼ãŒç•°ãªã‚‹
```

**çµæœ**: Hard Examples ãŒæ­£ã—ãåé›†ã•ã‚Œãšã€å®Ÿé¨“å¤±æ•—

### 2. Early Exit ã®å¿…é ˆä½¿ç”¨ (Stage 2 è©•ä¾¡)

**Stage 2 ã®æ¤œè¨¼æ™‚ã¯ Early Exit ã‚’å¿…ãšæœ‰åŠ¹åŒ–**:

```python
# âœ… æ­£ã—ã„å®Ÿè£…
eval_config = TrainingConfig(
    layer_weights={1: 0, 2: 0, 3: 0, 4: 1},
    routing_threshold=confidence_threshold,  # Early Exitæœ‰åŠ¹
    exit_layer=2
)
```

**ç†ç”±**: Early Exit ã‚’ä½¿ã‚ãªã„ã¨ Val PPL ãŒå˜èª¿æ¸›å°‘ã—ã€Early Stopping ãŒæ©Ÿèƒ½ã—ãªã„

**å®Ÿé¨“çµæœã¨ã®å¯¾å¿œ**:
- Early Exit ãªã— â†’ Val PPL: 987 â†’ 883 â†’ 845 â†’ ... (å˜èª¿æ¸›å°‘)
- Early Exit ã‚ã‚Š â†’ Val PPL: 987 â†’ 883 â†’ 845 â†’ 833 â†’ 830 â†’ 830 â†’ **830** (Epoch 7ã§ãƒ™ã‚¹ãƒˆ)

### 3. Early Stopping åˆ¤å®šåŸºæº–

**Stage 2 ã§ã¯ Val PPL ã‚’åŸºæº–ã«ã™ã‚‹** (Hard PPL ã§ã¯ãªã„):

```python
# âœ… æ­£ã—ã„å®Ÿè£…
if val_ppl < best_val_ppl:
    best_val_ppl = val_ppl
    save_model()
    patience_counter = 0
else:
    patience_counter += 1

# âŒ é–“é•ã£ãŸå®Ÿè£…
if hard_ppl < best_hard_ppl:  # â† Hard PPLã¯åˆ¤å®šåŸºæº–ã«ã—ãªã„
    ...
```

**ç†ç”±**: Val PPL ã¯æ±åŒ–æ€§èƒ½ã‚’è¡¨ã™ã€‚Hard PPL ã¯éå­¦ç¿’ã—ã‚„ã™ã„ã€‚

---

## ğŸ“Š å®Ÿé¨“çµæœã®æ¤œè¨¼æ–¹æ³•

### dont_delete.md ã¨ã®å®Œå…¨ä¸€è‡´ç¢ºèª

ä»¥ä¸‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒ **å®Œå…¨ã«ä¸€è‡´** ã™ã‚Œã°å®Ÿè£…ã¯æ­£ã—ã„:

#### Stage 1 (Phase 1)
```
âœ… Best Acc: 16.03%
âœ… Best PPL: 986.43
âœ… Best Epoch: 3
âœ… Early Stopping: Epoch 4
```

#### Hard Example Mining
```
âœ… Confidence Threshold: 0.1499
âœ… Collected Hard Examples: 32,768
âœ… Average Confidence: 0.0653
âœ… Actual Ratio: 51.2%
```

#### Stage 2 (Phase 2) - è¨“ç·´çµŒé
```
âœ… Epoch 5: Val PPL 829.80 (New best)
âœ… Epoch 7: Val PPL 829.78 (New best)
âœ… Epoch 8-10: No improvement (1/3, 2/3, 3/3)
âœ… Early Stopping: Epoch 10
âœ… Best Model: Epoch 7
```

#### Stage 2 (Phase 2) - æœ€çµ‚çµæœ
```
âœ… Best Val PPL: 829.78
âœ… Hard PPL: 668.08
âœ… Hard PPL Improvement: +2095.60 (+75.8%)
```

#### Final Evaluation (Two-Stage Inference)
```
âœ… Accuracy: 15.77%
âœ… Shallow ratio (Layer 2): 70.4%
âœ… Deep ratio (Layer 4): 29.6%
âœ… Compute cost: 64.82% of full model
```

### ä¸ä¸€è‡´ãŒç™ºç”Ÿã—ãŸå ´åˆã®ãƒ‡ãƒãƒƒã‚°

#### å•é¡Œ1: Hard Examples åé›†æ•°ãŒç•°ãªã‚‹
- **åŸå› **: Per-token filtering ãŒæ­£ã—ãå®Ÿè£…ã•ã‚Œã¦ã„ãªã„
- **ç¢ºèª**: `compute_confidence_threshold()` ã¨ `collect_hard_examples()` ã®æ–¹å¼ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹

#### å•é¡Œ2: Val PPL ãŒå˜èª¿æ¸›å°‘
- **åŸå› **: Early Exit ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹
- **ç¢ºèª**: Stage 2 è©•ä¾¡æ™‚ã® `TrainingConfig` ã« `routing_threshold` ã¨ `exit_layer` ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹

#### å•é¡Œ3: Early Stopping ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãŒç•°ãªã‚‹
- **åŸå› **: Early Stopping åˆ¤å®šåŸºæº–ãŒ Val PPL ã§ãªã„
- **ç¢ºèª**: `if val_ppl < best_val_ppl` ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹ (Hard PPL ã§ã¯ãªã„)

#### å•é¡Œ4: Hard PPL ã®æ”¹å–„ç‡ãŒç•°ãªã‚‹
- **åŸå› **: Hard Examples ã®è©•ä¾¡æ–¹æ³•ãŒé–“é•ã£ã¦ã„ã‚‹
- **ç¢ºèª**: `evaluate_on_hard_examples()` ãŒæ­£ã—ã„ `num_lower_layers` ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹

---

## ğŸ¯ Stage ã®æ¦‚å¿µçš„ç†è§£

### ASHEM = 2-Stage Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ASHEM Training Flow                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Stage 1 (Phase 1)
â”œâ”€ Input: å…¨ãƒ‡ãƒ¼ã‚¿ (100%)
â”œâ”€ Model: 2-layer Transformer
â”œâ”€ Output: è¨“ç·´æ¸ˆã¿æµ…å±¤ãƒ¢ãƒ‡ãƒ« + Confidence Threshold
â””â”€ Duration: ~3-4 epochs (Early Stopping: patience=1)

        â†“ (Hard Example Mining)

â”œâ”€ Compute Confidence Threshold (target_ratio=0.5)
â”œâ”€ Collect Hard Examples (ç´„50%ã®ãƒˆãƒ¼ã‚¯ãƒ³)
â””â”€ Identify: 32,768 hard tokens

        â†“

Stage 2 (Phase 2)
â”œâ”€ Input: Hard Examples ã®ã¿ (50%)
â”œâ”€ Model: 4-layer Transformer (Layer 1-2 frozen)
â”œâ”€ Training: Layer 3-4 ã®ã¿è¨“ç·´
â”œâ”€ Evaluation: Early Exit ä½¿ç”¨ (å…¨ãƒ‡ãƒ¼ã‚¿)
â”œâ”€ Output: è¨“ç·´æ¸ˆã¿æ·±å±¤ãƒ¢ãƒ‡ãƒ«
â””â”€ Duration: ~7-10 epochs (Early Stopping: patience=3)

        â†“

Final Inference (Two-Stage Routing)
â”œâ”€ Easy Examples â†’ Exit at Layer 2 (70.4%)
â”œâ”€ Hard Examples â†’ Process to Layer 4 (29.6%)
â””â”€ Compute Cost: 64.82% of full model
```

### Staged Deep Supervision (SDS) ã¸ã®æ‹¡å¼µ (æ¦‚å¿µã®ã¿)

**SDS** = N-Stage Training (ASHEMã®ä¸€èˆ¬åŒ–)

```
Stage 1: 2 layers, all data
   â†“
Stage 2: 4 layers, hard examples (threshold=0.5)
   â†“
Stage 3: 6 layers, very hard examples (threshold=0.2)
   â†“
...
```

**æ³¨æ„**: SDS ã®å®Ÿè£…ã¯æœªå®Œæˆã€‚ç¾åœ¨ã¯ ASHEM (2-Stage) ã®ã¿å‹•ä½œç¢ºèªæ¸ˆã¿ã€‚

---

## ğŸ”§ å®Ÿè£…ä¸Šã®æ¨å¥¨äº‹é …

### 1. ã‚³ãƒ¼ãƒ‰æ§‹é€ 

```python
# Stage 1
model_stage1 = StandardTransformer(num_layers=2)
result_stage1 = train_stage1(model_stage1, all_data)

# Hard Example Mining
threshold = compute_confidence_threshold(model_stage1, val_data, 0.5)
hard_examples = collect_hard_examples(model_stage1, val_data, threshold)

# Stage 2
model_stage2 = extend_model(model_stage1, num_layers=4)
freeze_lower_layers(model_stage2, num_lower_layers=2)
result_stage2 = train_stage2(model_stage2, hard_examples)

# Final Evaluation
stats = evaluate_two_stage(model_stage2, val_data, threshold)
```

### 2. è¨­å®šç®¡ç†

```python
@dataclass
class ASHEMConfig:
    # Stage 1
    phase1_layers: int = 2
    phase1_lr: float = 1e-3
    phase1_patience: int = 1

    # Hard Example Mining
    hard_example_ratio: float = 0.5

    # Stage 2
    phase2_layers: int = 4
    phase2_lr: float = 1e-4
    phase2_patience: int = 3
```

### 3. æ¤œè¨¼ã¨ãƒ­ã‚°

```python
# Stage 1 å®Œäº†æ™‚
print(f"Stage 1 - Best PPL: {phase1_ppl:.2f}")
print(f"Stage 1 - Hard PPL: {phase1_hard_ppl:.2f}")

# Hard Example Mining å®Œäº†æ™‚
print(f"Threshold: {threshold:.4f}")
print(f"Hard Examples: {len(hard_examples['targets']):,}")

# Stage 2 å„ã‚¨ãƒãƒƒã‚¯
print(f"Epoch {epoch} - Val PPL: {val_ppl:.2f} | Hard PPL: {hard_ppl:.2f}")

# æœ€çµ‚çµæœ
print(f"Hard PPL Improvement: {improvement:+.2f} ({improvement_pct:+.1f}%)")
```

---

## ğŸ“ ç”¨èªé›†

| ç”¨èª | å®šç¾© |
|-----|------|
| **Stage** | è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚ºã®æ™‚é–“çš„åŒºåˆ‡ã‚Šã€‚ç•°ãªã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§å®Ÿè¡Œã•ã‚Œã‚‹è¨“ç·´ã®æ®µéšã€‚ |
| **Phase** | Stage ã®åŒç¾©èª (æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯äº¤æ›å¯èƒ½) |
| **Hard Example** | ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ä¿¡é ¼åº¦ãŒé–¾å€¤ä»¥ä¸‹ã®ã‚µãƒ³ãƒ—ãƒ« (Per-token) |
| **Confidence Threshold** | Hard Example ã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã®ä¿¡é ¼åº¦ã®é–¾å€¤ |
| **Per-token Filtering** | å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å€‹åˆ¥ã«è©•ä¾¡ã—ã€é–¾å€¤æ¯”è¼ƒã‚’è¡Œã†æ–¹å¼ |
| **Hard Freezing** | `requires_grad=False` ã«ã‚ˆã‚‹å®Œå…¨ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‡çµ |
| **Early Exit** | æ¨è«–æ™‚ã«é€”ä¸­ã®å±¤ã§å‡¦ç†ã‚’çµ‚äº†ã™ã‚‹æ©Ÿæ§‹ |
| **Two-Stage Routing** | Easy examples ã¯æµ…å±¤ã§ã€Hard examples ã¯æ·±å±¤ã§å‡¦ç† |

---

## âœ… ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

å®Ÿè£…æ™‚ã«å¿…ãšç¢ºèªã™ã‚‹é …ç›®:

### Stage 1
- [ ] ãƒ¢ãƒ‡ãƒ«ã¯2å±¤
- [ ] å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
- [ ] Early Stopping: patience=1
- [ ] Learning Rate: 1e-3

### Hard Example Mining
- [ ] `compute_confidence_threshold()`: Per-token quantile
- [ ] `collect_hard_examples()`: Per-token filtering
- [ ] Thresholdè¨ˆç®—ã¨Filteringæ–¹å¼ãŒä¸€è‡´
- [ ] ç´„50%ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒåé›†ã•ã‚Œã‚‹

### Stage 2
- [ ] ãƒ¢ãƒ‡ãƒ«ã¯4å±¤ (Stage 1ã‹ã‚‰æ‹¡å¼µ)
- [ ] Layer 1-2: é‡ã¿ã‚³ãƒ”ãƒ¼ + Frozen (`requires_grad=False`)
- [ ] Layer 3-4: ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ– + Trainable
- [ ] **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: Hard Examples ã®ã¿
- [ ] **æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿**: å…¨ãƒ‡ãƒ¼ã‚¿ (Early Exit ä½¿ç”¨)
- [ ] Early Stopping: patience=3, Val PPLåŸºæº–
- [ ] Learning Rate: 1e-4

### Final Evaluation
- [ ] Early Exit æœ‰åŠ¹
- [ ] Shallow ratio è¨ˆç®—
- [ ] Compute cost è¨ˆç®—

---

## ğŸš¨ çµ¶å¯¾ã«å®ˆã‚‹ã¹ããƒ«ãƒ¼ãƒ«

1. âœ… **Per-token Filtering ã®ä¸€è²«æ€§**: Thresholdè¨ˆç®—ã¨Filteringã§åŒã˜æ–¹å¼ã‚’ä½¿ç”¨
2. âœ… **Early Exit ã®å¿…é ˆä½¿ç”¨**: Stage 2 è©•ä¾¡æ™‚ã¯å¿…ãš Early Exit ã‚’æœ‰åŠ¹åŒ–
3. âœ… **Val PPL åŸºæº–ã® Early Stopping**: Hard PPL ã§ã¯ãªã Val PPL ã§åˆ¤å®š
4. âœ… **Hard Freezing ã®ç¢ºèª**: Layer 1-2 ãŒ `requires_grad=False` ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
5. âœ… **ãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºãªä½¿ç”¨**: Stage 1=å…¨ãƒ‡ãƒ¼ã‚¿ã€Stage 2=Hard Examples ã®ã¿

---

## ğŸ“š å‚è€ƒãƒªãƒ³ã‚¯

- å®Ÿè£…ã‚³ãƒ¼ãƒ‰: [colab2.py](../colab2.py)
- ASHEM ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: [src/ease/ashem.py](../src/ease/ashem.py)
- å®Ÿé¨“çµæœ: [dont_delete.md](../dont_delete.md)
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦: [CLAUDE.md](../CLAUDE.md)

---

**Last Updated**: 2025-12-12
**Verified**: Commit fc9b140 (å‹•ä½œç¢ºèªæ¸ˆã¿)
