# HRM Project - Claude Instructions

## Project Overview

**EASE: Efficient Asymmetric Supervision for Early-Exit Transformers**

Early-Exit Transformer ã®å­¦ç¿’æ–¹æ³•ã«é–¢ã™ã‚‹ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‚

---

## è«–æ–‡æ–¹é‡

### ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å
**EASE** (Efficient Asymmetric Supervision for Early-Exit)

### è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«æ¡ˆ
> "EASE: Efficient Asymmetric Supervision for Early-Exit Transformers"

ã¾ãŸã¯

> "Rethinking Auxiliary Loss for Early-Exit Transformers: Why Intermediate Layers Should Not Predict"

### ä¸»è¦ãªè²¢çŒ®

1. **ä¸­é–“å±¤æå¤±ã‚¼ãƒ­ã®ç™ºè¦‹**
   - L2ï¼ˆä¸­é–“å±¤ï¼‰ã«æå¤±ã‚’é©ç”¨ã™ã‚‹ã¨ 39.8% æ€§èƒ½æ‚ªåŒ–
   - ä¸­é–“å±¤ã¯ã€Œç´”ç²‹ãªç‰¹å¾´æŠ½å‡ºå±¤ã€ã¨ã—ã¦æ©Ÿèƒ½ã•ã›ã‚‹ã¹ã
   - Confidence Calibration ã®æ”¹å–„ã«ã¤ãªãŒã‚‹

2. **éå¯¾ç§°æå¤±é‡ã¿ä»˜ã‘ï¼ˆAsymmetric Auxiliary Lossï¼‰**
   - Î±=0.7ï¼ˆShallowé‡è¦–ï¼‰ãŒæœ€é©
   - æ—¢å­˜ç ”ç©¶ã® Î±=0.5ï¼ˆå‡ç­‰ï¼‰ã‚ˆã‚Š 4.3% æ”¹å–„

3. **Discriminative Fine-Tuning Ã— Early Exit ã®æ–°çµ„ã¿åˆã‚ã›**
   - æµ…ã„å±¤ã«é«˜LRã€æ·±ã„å±¤ã«ä½LR
   - 46.9% æ”¹å–„ï¼ˆæœ€è‰¯çµæœï¼‰

4. **Universal Training Framework**
   - Deep Supervision, Auxiliary Loss, Early Exit, Discriminative FT ã‚’çµ±ä¸€çš„ã«è¡¨ç¾

### è«–æ–‡æ§‹æˆ

```
1. Introduction
   - Early Exit ã®é‡è¦æ€§ã¨èª²é¡Œ
   - å­¦ç¿’æ–¹æ³•ã®ä½“ç³»çš„ç ”ç©¶ã®ä¸è¶³

2. Universal Training Framework (EASE)
   - æ—¢å­˜æ‰‹æ³•ã®çµ±ä¸€çš„è¡¨ç¾
   - layer_weights, routing_threshold, layer_lr_scales

3. Experiments
   3.1 ä¸­é–“å±¤æå¤±ã®å½±éŸ¿ (L2=0 vs L2>0)
   3.2 éå¯¾ç§°æå¤±é‡ã¿ä»˜ã‘ (Î±æ¢ç´¢)
   3.3 Discriminative Fine-Tuning

4. Analysis
   - ãªãœä¸­é–“å±¤æå¤±0ãŒåŠ¹ãã‹
   - Confidence Calibration ã¨ã®é–¢ä¿‚

5. Related Work
   - Deep Supervision (Lee et al., 2015)
   - Early Exit (BranchyNet, CALM, LayerSkip)
   - Discriminative Fine-Tuning (ULMFiT)

6. Conclusion
```

### æŠ•ç¨¿å…ˆå€™è£œ

| å„ªå…ˆåº¦ | ä¼šè­°/ã‚¸ãƒ£ãƒ¼ãƒŠãƒ« | ç†ç”± |
|--------|----------------|------|
| 1 | arXiv (ãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆ) | ã¾ãšå…¬é–‹ã—ã¦åå¿œã‚’è¦‹ã‚‹ |
| 2 | EMNLP Findings | åŠ¹ç‡çš„NLPã«é–¢å¿ƒé«˜ã„ |
| 3 | ACL Findings | åŒä¸Š |

---

## ç”¨èªå¯¾å¿œè¡¨

| ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†… (æ—§) | å­¦è¡“ç”¨èª | Reference |
|-------------------|---------|-----------|
| LPT | **Deep Supervision** | Lee et al., 2015 |
| Standard Routing | **Auxiliary Loss Training** | Elbayad et al., 2020 |
| Confidence-based Routing | **Early Exit** | Teerapittayanon et al., 2016 |
| Layer-wise Learning Rate | **Discriminative Fine-Tuning** | Howard & Ruder, 2018 |
| Dynamic Alpha | **Learning Rate Curriculum** | Croitoru et al., 2024 |

---

## ä¸»è¦ãªå®Ÿé¨“çµæœ

| Rank | Model | PPL | vs Standard 3L |
|------|-------|-----|----------------|
| ğŸ¥‡ | Discriminative FT (Decreasing LR) | 18.52 | **46.9% æ”¹å–„** |
| ğŸ¥ˆ | Asymmetric Auxiliary Loss (Î±=0.7) | 22.95 | 34.2% æ”¹å–„ |
| ğŸ¥‰ | Auxiliary Loss (Î±=0.5) | 23.98 | 31.2% æ”¹å–„ |
| - | Standard (3L) | 34.86 | (baseline) |

**é‡è¦ãªç™ºè¦‹**: L2ãƒ­ã‚¹è¿½åŠ ã§ **39.8% æ‚ªåŒ–** (22.95 â†’ 32.07)

---

## é–¢é€£ç ”ç©¶ã¨ã®ä½ç½®ã¥ã‘

| ç ”ç©¶ | ç„¦ç‚¹ | EASE ã¨ã®é•ã„ |
|------|------|--------------|
| CALM (Google, 2022) | æ¨è«–æ™‚ã®åˆ¤å®šæ–¹æ³• | å­¦ç¿’æ™‚ã®æå¤±è¨­è¨ˆã«æ³¨ç›® |
| LayerSkip (Meta, 2024) | Layer Dropout + æ¨è«– | æå¤±ã®æœ€é©é…ç½®ã‚’ç™ºè¦‹ |
| EE-LLM (Alibaba, 2023) | ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ | ä¸­é–“å±¤æå¤±0ã®é‡è¦æ€§ã‚’ç™ºè¦‹ |

---

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
hrm/
â”œâ”€â”€ CLAUDE.md                    # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ src/
â”‚   â””â”€â”€ ease/                    # EASE ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ (pip installable)
â”‚       â”œâ”€â”€ __init__.py          # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
â”‚       â”œâ”€â”€ models.py            # StandardTransformer, ConfidenceRoutedTransformer
â”‚       â”œâ”€â”€ trainer.py           # UniversalConfig, UniversalTrainer, AlphaSchedule
â”‚       â””â”€â”€ modules/             # ã‚³ã‚¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚           â”œâ”€â”€ norm.py          # RMSNorm
â”‚           â”œâ”€â”€ attention.py     # MultiHeadAttention, RoPE
â”‚           â”œâ”€â”€ ffn.py           # GatedLinearUnit
â”‚           â””â”€â”€ transformer.py   # TransformerBlock
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ utils.py                 # ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€ã‚·ãƒ¼ãƒ‰è¨­å®š
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ REFERENCES.md            # å­¦è¡“çš„å‚è€ƒæ–‡çŒ®
â”‚   â””â”€â”€ experiments/             # å®Ÿé¨“çµæœãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
â””â”€â”€ run_experiments.py           # å®Ÿé¨“å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆè–„ã„ãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰
```

### ä½¿ç”¨æ–¹æ³•

```python
import sys
sys.path.insert(0, 'src')

from ease import (
    ConfidenceRoutedTransformer,
    UniversalConfig,
    UniversalTrainer,
    PRESETS,
)

# ãƒ—ãƒªã‚»ãƒƒãƒˆä½¿ç”¨
config = PRESETS['asymmetric']  # Î±=0.7, L2=0

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
config = UniversalConfig(
    layer_weights={1: 0.7, 2: 0, 3: 0.3},
    routing_threshold=0.95,
)

# ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆ
model = ConfidenceRoutedTransformer(vocab_size=1000, dim=64, num_layers=3)
trainer = UniversalTrainer(config, vocab_size=1000)
```

---

## ä»Šå¾Œã®ã‚¿ã‚¹ã‚¯

- [ ] è«–æ–‡åŸ·ç­†ï¼ˆarXiv æŠ•ç¨¿ç”¨ï¼‰
- [ ] ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã§ã®æ¤œè¨¼å®Ÿé¨“
- [ ] å®Ÿéš›ã® LLM (Llama ç­‰) ã§ã®æ¤œè¨¼
- [ ] LayerSkip ã¨ã®çµ„ã¿åˆã‚ã›å®Ÿé¨“
