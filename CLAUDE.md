# HRM Project - Claude Instructions

## Project Overview

**LEGO: Layered Ensemble with Gradual Optimization**

ãƒ¬ã‚´ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚ˆã†ã«Stageï¼ˆå±¤ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰ã‚’çµ„ã¿åˆã‚ã›ã‚‹æŸ”è»Ÿãªè¨“ç·´ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚

**ã‚³ã‚¢æŠ€è¡“**:
- **Stage-based Training**: å±¤ã‚’Stageã¨ã„ã†ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã—ã€æŸ”è»Ÿã«çµ„ã¿åˆã‚ã›ã‚‹
- **ASHEM (Adaptive Supervision via Hard Example Mining)**: Hard examplesã«ç‰¹åŒ–ã—ãŸ2-Stageè¨“ç·´æˆ¦ç•¥
- **Early Exit**: æ¨è«–æ™‚ã®è¨ˆç®—åŠ¹ç‡åŒ–

**å®Ÿè£…çŠ¶æ³**:
- âœ… 2-Stage LEGO (ASHEM): å®Ÿè£…å®Œæˆã€å‹•ä½œç¢ºèªæ¸ˆã¿ ([docs/ASHEM_STAGE_SPECIFICATION.md](docs/ASHEM_STAGE_SPECIFICATION.md))
- ğŸ”„ N-Stage LEGO: æ¦‚å¿µææ¡ˆæ¸ˆã¿ã€å®Ÿè£…äºˆå®š

---

## ğŸš¨ é‡è¦ãªå®Ÿè£…ä¸Šã®æ³¨æ„äº‹é …

### Per-token Filtering - ASHEMå®Ÿè£…ã®å¿…é ˆä»•æ§˜

**âš ï¸ CRITICAL**: ASHEMã®Hard Example Miningã§ã¯**Per-token filtering**ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒå¿…é ˆã§ã™ã€‚

**å‹•ä½œã™ã‚‹å®Ÿè£…**: ã‚³ãƒŸãƒƒãƒˆ **fc9b140** (Consolidate LASH to 2 core options)
- `src/ease/ashem.py`: Per-token filteringå®Ÿè£…
- `colab2.py`: å‹•ä½œç¢ºèªæ¸ˆã¿å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**Per-token filteringã®å®Ÿè£…**:
```python
def compute_confidence_threshold(model, val_batches, target_ratio, device):
    """Per-token quantile calculation"""
    all_confidences = []
    for x, _ in val_batches:
        h = model.forward_to_layer(x, model.num_layers)
        confidence = compute_confidence(model, h)
        all_confidences.append(confidence.view(-1))  # â† Flatten per-token

    all_confidences = torch.cat(all_confidences)
    threshold = torch.quantile(all_confidences, target_ratio).item()
    return threshold

def collect_hard_examples(model, val_batches, threshold, device):
    """Per-token filtering"""
    for x, y in val_batches:
        h = model.forward_to_layer(x, model.num_layers)
        confidence = compute_confidence(model, h)

        # Per-token comparison
        mask = confidence < threshold  # (batch, seq_len)

        x_flat = x.view(-1)
        h_flat = h.view(-1, h.shape[-1])
        y_flat = y.view(-1)
        mask_flat = mask.view(-1)

        hard_inputs.append(x_flat[mask_flat])
        hard_hidden_states.append(h_flat[mask_flat])
        hard_targets.append(y_flat[mask_flat])
```

**æœŸå¾…ã•ã‚Œã‚‹å®Ÿé¨“çµæœ** (WikiText-2, 10K samples):
- Stage 1 Hard PPL: ~2,763
- Stage 2 Hard PPL: ~668
- Hard PPL Improvement: 75.8%
- Collected hard examples: ~32,768 (50% of total tokens)

**ç¦æ­¢äº‹é …**:
- âŒ Sequence-level averaging (`.mean(dim=1)`) ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨
- âŒ Per-token thresholdã¨Sequence-level averageã‚’æ··åœ¨ã•ã›ã‚‹ã“ã¨

**ç†ç”±**: Thresholdè¨ˆç®—ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®æ–¹æ³•ãŒä¸€è‡´ã—ã¦ã„ãªã„ã¨ã€hard examplesãŒæ­£ã—ãåé›†ã•ã‚Œãšã€å®Ÿé¨“ãŒå¤±æ•—ã—ã¾ã™ã€‚

---

## LEGO ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

### ã‚³ã‚¢ã‚³ãƒ³ã‚»ãƒ—ãƒˆ: ãƒ¬ã‚´ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚ˆã†ãªçµ„ã¿åˆã‚ã›

**LEGO**ã®2ã¤ã®ã‚³ã‚¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å…¨ã¦ã‚’åˆ¶å¾¡ï¼š

| ã‚ªãƒ—ã‚·ãƒ§ãƒ³ | èª¬æ˜ | Reference |
|-----------|------|-----------|
| **stages** | ã©ã®Stageãƒ–ãƒ­ãƒƒã‚¯ã§å­¦ç¿’ã™ã‚‹ã‹ | - |
| **routing_threshold** | æ¨è«–æ™‚Early Exité–¾å€¤ | Teerapittayanon et al., 2016 |

**é‡è¦**: Standard, Deep Supervision, ASHEMã¯å…¨ã¦LEGOã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç•°ãªã‚‹çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‚

### è¨­å®šä¾‹ï¼šæŸ”è»Ÿãªçµ„ã¿åˆã‚ã›

#### ãƒ‘ã‚¿ãƒ¼ãƒ³1: Standard Transformerï¼ˆå¾“æ¥å‹LLMï¼‰
```python
from ease import TrainingConfig, StageConfig

config = TrainingConfig(
    stages=[
        StageConfig(layers=(3, 3), loss_weight=1.0)  # æœ€çµ‚å±¤ã®ã¿ï¼ˆ1 stageï¼‰
    ]
)
```

#### ãƒ‘ã‚¿ãƒ¼ãƒ³2: Deep Supervisionï¼ˆå…¨å±¤å‡ç­‰ï¼‰
```python
config = TrainingConfig(
    stages=[
        StageConfig(layers=(1, 1), loss_weight=0.33),  # Layer 1
        StageConfig(layers=(2, 2), loss_weight=0.33),  # Layer 2
        StageConfig(layers=(3, 3), loss_weight=0.33),  # Layer 3
    ]
)
```

#### ãƒ‘ã‚¿ãƒ¼ãƒ³3: ASHEMï¼ˆ2-Stageè¨“ç·´ï¼‰
```python
# Stage 1: Layer 1-2, Stage 2: Layer 3-4
config = TrainingConfig(
    stages=[
        StageConfig(layers=(1, 2), loss_weight=1.0),  # Stage 1: æµ…å±¤
        StageConfig(layers=(3, 4), loss_weight=1.0),  # Stage 2: æ·±å±¤
    ],
    routing_threshold=0.95,  # æ¨è«–æ™‚Early Exit
    exit_layer=2
)
```

#### ãƒ‘ã‚¿ãƒ¼ãƒ³4: ã‚«ã‚¹ã‚¿ãƒ ï¼ˆéå¯¾ç§°é‡ã¿ï¼‰
```python
# Layer 1-2ã«é‡ç‚¹ã€Layer 3ã¯è»½ã‚
config = TrainingConfig(
    stages=[
        StageConfig(layers=(1, 2), loss_weight=0.7),
        StageConfig(layers=(3, 3), loss_weight=0.3),
    ],
    routing_threshold=0.9,
    exit_layer=2
)
```

---

## ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç®¡ç†ãƒãƒªã‚·ãƒ¼

### ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å‰Šé™¤ç†ç”±

**CLAUDE.mdã«ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯è¨˜è¼‰ã—ãªã„**

**ç†ç”±**:
- ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã¯é »ç¹ã«å¤‰æ›´ã•ã‚Œã‚‹ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã€æ–°æ©Ÿèƒ½è¿½åŠ ç­‰ï¼‰
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ›´æ–°å¿˜ã‚Œã«ã‚ˆã‚‹æƒ…å ±ã®é™³è…åŒ–ã‚’é˜²ã
- å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚’è¦‹ã‚Œã°æ§‹æˆã¯æŠŠæ¡ã§ãã‚‹
- Globãƒ„ãƒ¼ãƒ«ã§ç°¡å˜ã«ç¢ºèªå¯èƒ½: `**/*.py`

**æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:
- é‡è¦ãªã®ã¯ã€Œä½¿ã„æ–¹ã€ã¨ã€Œæ¦‚å¿µã€
- ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€ã¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¾‹ã§ååˆ†
- æ§‹é€ çš„ãªèª¬æ˜ãŒå¿…è¦ãªå ´åˆã¯ã€ã‚³ãƒ¡ãƒ³ãƒˆã‚„docstringã«è¨˜è¼‰

---

## ã‚³ãƒ¼ãƒ‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆ

### LEGO ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ (src/ease/)

**ã‚³ã‚¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«**:
- `models.py` - StandardTransformer, DeepSupervisionTransformer
- `trainer.py` - StageConfig, TrainingConfig, Trainer (Stage-basedè¨“ç·´ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯)
- `ashem.py` - ASHEMConfig, ASHEMè¨“ç·´æˆ¦ç•¥ï¼ˆPer-token filteringå®Ÿè£…ï¼‰
- `modules/` - TransformerBlock, Attention, FFN, RMSNormç­‰

**å®Ÿé¨“ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ (experiments/)**:
- `utils.py` - ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã€ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ã€seedè¨­å®š

**å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (root)**:
- `colab2.py` - ASHEMå®Ÿé¨“ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆfc9b140ã§å‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰

---

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹

```python
import sys
sys.path.insert(0, 'src')

from ease import DeepSupervisionTransformer, Trainer, TrainingConfig, StageConfig

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
model = DeepSupervisionTransformer(vocab_size=1000, dim=64, num_layers=3)

# è¨­å®š: LASHã®2ã¤ã®ã‚³ã‚¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å…¨ã¦ã‚’åˆ¶å¾¡
config = TrainingConfig(
    stages=[
        StageConfig(layers=(1, 2), loss_weight=0.7),  # Stage 1: Layer 1-2
        StageConfig(layers=(3, 3), loss_weight=0.3),  # Stage 2: Layer 3
    ],
    routing_threshold=0.95,  # Early Exité–¾å€¤
    exit_layer=2
)

# è¨“ç·´
trainer = Trainer(config, vocab_size=1000)
optimizer = trainer.create_optimizer(model, base_lr=1e-3)
loss = trainer.train_epoch(model, train_batches, optimizer)

# è©•ä¾¡
stats = trainer.evaluate(model, val_batches)
```

### ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆä¾¿åˆ©ãªè¨­å®šãƒ—ãƒªã‚»ãƒƒãƒˆï¼‰

```python
from ease import create_standard_config, create_deep_supervision_config

# Standard LLMè¨­å®šï¼ˆæœ€çµ‚å±¤ã®ã¿ï¼‰
config = create_standard_config(num_layers=3)
# â†’ stages=[StageConfig(layers=(3, 3), loss_weight=1.0)]

# Deep Supervisionè¨­å®šï¼ˆå…¨å±¤å‡ç­‰ï¼‰
config = create_deep_supervision_config(num_layers=3)
# â†’ stages=[StageConfig(layers=(1, 1), 0.33), StageConfig(layers=(2, 2), 0.33), StageConfig(layers=(3, 3), 0.33)]
```

**æ³¨æ„**: ã“ã‚Œã‚‰ã¯ã‚ãã¾ã§ãƒ—ãƒªã‚»ãƒƒãƒˆã€‚`TrainingConfig`ã§è‡ªç”±ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ã€‚

### Early Stoppingï¼ˆè¨“ç·´æ™‚ã®æ—©æœŸçµ‚äº†ï¼‰

```python
# Early Stoppingä»˜ãè¨“ç·´
result = trainer.train_with_early_stopping(
    model=model,
    train_batches=train_loader,
    val_batches=val_loader,
    optimizer=optimizer,
    max_epochs=100,
    patience=1,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤: 1ã‚¨ãƒãƒƒã‚¯æ”¹å–„ãªã—ã§åœæ­¢
    verbose=True
)
```

**é‡è¦ãƒ«ãƒ¼ãƒ«**:
- **patienceã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯1**
- æ¤œè¨¼æå¤±ãŒ1ã‚¨ãƒãƒƒã‚¯ã§ã‚‚æ‚ªåŒ–ã—ãŸã‚‰è¨“ç·´ã‚’åœæ­¢
- éå­¦ç¿’ã‚’é˜²ãã€è¨“ç·´æ™‚é–“ã‚’çŸ­ç¸®
- æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ã‚’è‡ªå‹•ä¿å­˜ãƒ»å¾©å…ƒ

### Perplexity (PPL) ã®è§£é‡ˆ

**æ­£å¸¸ãªå€¤ã®ç¯„å›²**:
- **å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ1K ã‚µãƒ³ãƒ—ãƒ«ï¼‰**: PPL 100-3000 ç¨‹åº¦
- **ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ10K ã‚µãƒ³ãƒ—ãƒ«ï¼‰**: PPL 10-1000 ç¨‹åº¦
- **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿**: PPL 2-100 ç¨‹åº¦

**è¨ˆç®—å¼**: `PPL = exp(avg_loss)`
- vocab_size=1000ã®ãƒ©ãƒ³ãƒ€ãƒ äºˆæ¸¬: loss â‰ˆ log(1000) â‰ˆ 6.9, PPL â‰ˆ 1000
- loss=7.3 â†’ PPL â‰ˆ 1500ï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯æ­£å¸¸ï¼‰
- loss=2.3 â†’ PPL â‰ˆ 10ï¼ˆååˆ†ã«å­¦ç¿’æ¸ˆã¿ï¼‰

**æ³¨æ„**: PPLã¯æŒ‡æ•°é–¢æ•°çš„ã«å¢—åŠ ã™ã‚‹ãŸã‚ã€lossãŒã‚ãšã‹ã«é«˜ã„ã ã‘ã§PPLã¯å¤§ããè¦‹ãˆã¾ã™ã€‚**Accuracyã§è©•ä¾¡**ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã€‚

---

## LEGO è¨“ç·´æˆ¦ç•¥

LEGOã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯3ã¤ã®è¨“ç·´æˆ¦ç•¥ã‚’ã‚µãƒãƒ¼ãƒˆï¼š

### 1. Standard LEGO
æœ€çµ‚å±¤ã®ã¿ã§å­¦ç¿’ï¼ˆå¾“æ¥ã®LLMè¨“ç·´ï¼‰= **1ã¤ã®Stageãƒ–ãƒ­ãƒƒã‚¯**
```python
config = TrainingConfig(stages=[
    StageConfig(layers=(3, 3), loss_weight=1.0)  # æœ€çµ‚å±¤ã®ã¿ã®1ãƒ–ãƒ­ãƒƒã‚¯
])
```

### 2. Deep Supervision LEGO
å…¨å±¤ã§å‡ç­‰ã«å­¦ç¿’ = **å…¨å±¤ã‚’å€‹åˆ¥Stageãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦å®šç¾©**
```python
config = TrainingConfig(stages=[
    StageConfig(layers=(1, 1), loss_weight=0.33),  # ãƒ–ãƒ­ãƒƒã‚¯1
    StageConfig(layers=(2, 2), loss_weight=0.33),  # ãƒ–ãƒ­ãƒƒã‚¯2
    StageConfig(layers=(3, 3), loss_weight=0.33),  # ãƒ–ãƒ­ãƒƒã‚¯3
])
```

### 3. ASHEM LEGO
Hard examplesã«ç‰¹åŒ–ã—ãŸ**2-Stageãƒ–ãƒ­ãƒƒã‚¯è¨“ç·´æˆ¦ç•¥**

**æ–°è¦æ€§**: ä¸¡ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ï¼ˆ2024-2025ï¼‰ã«Early Exitã¨Hard Example Miningã®çµ„ã¿åˆã‚ã›ã«é–¢ã™ã‚‹è¨˜è¿°ãªã—

**è¨“ç·´æ‰‹é †**:
- **Stage 1 Block**: æµ…å±¤ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆLayer 1-2ï¼‰ã§å…¨ãƒ‡ãƒ¼ã‚¿è¨“ç·´ â†’ Hard exampleè­˜åˆ¥
- **Stage 2 Block**: æ·±å±¤ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆLayer 3-4ï¼‰ã§Hard examplesã®ã¿è¨“ç·´
- **æ¨è«–**: 2ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆï¼ˆEarly Exitï¼‰

**LEGOãƒ–ãƒ­ãƒƒã‚¯ã®çµ„ã¿åˆã‚ã›**:
```python
# ãƒ–ãƒ­ãƒƒã‚¯1: Layer 1-2ï¼ˆæµ…å±¤ï¼‰
# ãƒ–ãƒ­ãƒƒã‚¯2: Layer 3-4ï¼ˆæ·±å±¤ã€Hard examplesã®ã¿ï¼‰
config = TrainingConfig(
    stages=[
        StageConfig(layers=(1, 2), loss_weight=1.0),  # ãƒ–ãƒ­ãƒƒã‚¯1
        StageConfig(layers=(3, 4), loss_weight=1.0),  # ãƒ–ãƒ­ãƒƒã‚¯2
    ],
    routing_threshold=0.15,  # æ¨è«–æ™‚ãƒ–ãƒ­ãƒƒã‚¯åˆ‡ã‚Šæ›¿ãˆé–¾å€¤
    exit_layer=2
)
```

**å®Ÿé¨“çµæœ** (WikiText-2, 10K samples):
- Hard PPL: **75.8%æ”¹å–„** (2763 â†’ 668)
- è¨ˆç®—ã‚³ã‚¹ãƒˆ: **36%å‰Šæ¸›** (64.82% of full model)
- Overall PPL: **15.9%æ”¹å–„** (986 â†’ 830)

**ä½¿ç”¨ä¾‹**:
```python
from ease import ASHEMConfig

ashem_config = ASHEMConfig(
    phase1_layers=2,        # Stage 1ã®å±¤æ•°
    hard_example_ratio=0.5, # Hard exampleåé›†ç‡
    phase2_layers=4,        # Stage 2ã®ç·å±¤æ•°
)
```

è©³ç´°: [docs/experiments/hard_example_mining.md](docs/experiments/hard_example_mining.md)

**ASHEM ã®è©³ç´°ä»•æ§˜**: [docs/ASHEM_STAGE_SPECIFICATION.md](docs/ASHEM_STAGE_SPECIFICATION.md)
- Stage (ã‚¹ãƒ†ãƒ¼ã‚¸) ã®æ­£ç¢ºãªå®šç¾©
- Per-token Filtering ã®å¿…é ˆä»•æ§˜
- Early Exit ã®å¿…é ˆä½¿ç”¨
- å®Ÿé¨“çµæœã®æ¤œè¨¼æ–¹æ³•

**æ³¨æ„**: SDS ã®å®Ÿè£…ã¯æœªå®Œæˆã€‚ç¾åœ¨ã¯ ASHEM (2-Stage) ã®ã¿å‹•ä½œç¢ºèªæ¸ˆã¿ (commit fc9b140)ã€‚

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### compute_loss() ã®è‡ªå‹•æœ€é©åŒ–

**LASHã®2ã¤ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å®Œå…¨ã«ç¶­æŒã—ãŸã¾ã¾ã€è¨“ç·´é€Ÿåº¦ã‚’æœ€é©åŒ–**:

```python
# æœ€çµ‚å±¤ã®ã¿ï¼ˆé«˜é€Ÿãƒ‘ã‚¹ä½¿ç”¨ï¼‰
config = TrainingConfig(layer_weights={1: 0, 2: 0, 3: 1})
# â†’ forward() ã‚’ä½¿ç”¨ï¼ˆç´„8%é«˜é€ŸåŒ–ï¼‰

# è¤‡æ•°å±¤ï¼ˆæ±ç”¨ãƒ‘ã‚¹ä½¿ç”¨ï¼‰
config = TrainingConfig(layer_weights={1: 0.33, 2: 0.33, 3: 0.33})
# â†’ forward_all_layers() ã‚’ä½¿ç”¨

# éå¯¾ç§°ï¼ˆæ±ç”¨ãƒ‘ã‚¹ä½¿ç”¨ï¼‰
config = TrainingConfig(layer_weights={1: 0.7, 2: 0, 3: 0.3})
# â†’ forward_all_layers() ã‚’ä½¿ç”¨
```

**æœ€é©åŒ–ã®ä»•çµ„ã¿**:
- `layer_weights` ã‚’è§£æã—ã€æœ€çµ‚å±¤ã®ã¿å¿…è¦ãªå ´åˆã‚’æ¤œå‡º
- æœ€çµ‚å±¤ã®ã¿ã®å ´åˆ â†’ `forward()` ä½¿ç”¨ï¼ˆä¸­é–“å±¤ã§output_headã‚’å®Ÿè¡Œã—ãªã„ï¼‰
- ãã‚Œä»¥å¤– â†’ `forward_all_layers()` ä½¿ç”¨ï¼ˆå¾“æ¥é€šã‚Šï¼‰

**äº’æ›æ€§ä¿è¨¼**:
- âœ… `layer_weights`: ã™ã¹ã¦ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å‹•ä½œ
- âœ… `routing_threshold`: ç‹¬ç«‹ï¼ˆè©•ä¾¡æ™‚ã®ã¿ä½¿ç”¨ï¼‰

**å®Ÿæ¸¬åŠ¹æœ**ï¼ˆWikiText-2, 10K samplesï¼‰:
- æœ€çµ‚å±¤ã®ã¿: **8.4%é«˜é€ŸåŒ–**ï¼ˆ25.51ç§’ â†’ 23.38ç§’ï¼‰
- è¤‡æ•°å±¤: å¤‰åŒ–ãªã—ï¼ˆã™ã§ã«æœ€é©ï¼‰

---

## References

### LASH Framework
- **LASH**: Layered Adaptive Supervision Hierarchyï¼ˆæœ¬ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰
- Lee et al. (2015) - Deep Supervision
- Howard & Ruder (2018) - Discriminative Fine-Tuning
- Teerapittayanon et al. (2016) - Early Exit (BranchyNet)

### ASHEM Training Strategy
- **ASHEM**: Adaptive Supervision via Hard Example Miningï¼ˆæœ¬ç ”ç©¶ï¼‰
- Hard Example Mining: Similar to HAM (IEEE TIFS 2025), HSM (2025)
- **æ³¨æ„**: "Progressive Layer Addition"ã§ã¯ãªã"Selective Layer Expansion"ã‚’ä½¿ç”¨ï¼ˆPLD (NeurIPS 2020)ã¨ã®æ··åŒã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰

### Early Exit Surveys (æ–°è¦æ€§æ¤œè¨¼ç”¨)
- **ACM Survey** (Nov 2024): "Early-Exit Deep Neural Networkâ€”A Comprehensive Survey" (37 pages)
  - Haseena Rahmath P et al., ACM Computing Surveys
  - DOI: 10.1145/3698767
- **NLP Survey** (Jan 2025): "A Survey of Early Exit Deep Neural Networks in NLP" (13 pages)
  - Divya Jyoti Bajpai and Manjesh Kumar Hanawal
  - arXiv:2501.07670v1

**é‡è¦ãªçŸ¥è¦‹**:
- ä¸¡ã‚µãƒ¼ãƒ™ã‚¤ã¨ã‚‚ã€Early Exitã®æ–‡è„ˆã§ã®å±¤ã”ã¨ã®å­¦ç¿’ç‡åˆ¶å¾¡ï¼ˆ`layer_lr_scales`ï¼‰ã«è¨€åŠãªã—
- ä¸¡ã‚µãƒ¼ãƒ™ã‚¤ã¨ã‚‚ã€Early Exitã¨Hard Example Miningã®çµ„ã¿åˆã‚ã›ã«è¨€åŠãªã—
- æ—¢å­˜ç ”ç©¶ã§ã¯`wi = i`ï¼ˆæ·±ã„å±¤ã»ã©é‡ã¿ãŒå¤§ãã„ï¼‰ãŒä¸€èˆ¬çš„

---

## ã‚³ãƒ¼ãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†é›¢åŸå‰‡

**è¨“ç·´ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨è¨“ç·´æˆ¦ç•¥ã®åˆ†é›¢**:
- `trainer.py` - ã‚³ã‚¢è¨“ç·´ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆTrainingConfig, Trainerï¼‰
- `ashem.py` - ASHEMè¨“ç·´æˆ¦ç•¥å°‚ç”¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆPer-token filteringå®Ÿè£…ï¼‰

**åˆ†é›¢ã®åˆ©ç‚¹**:
- æ˜ç¢ºãªè²¬å‹™åˆ†é›¢: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ vs æˆ¦ç•¥
- æ‹¡å¼µæ€§: æ–°ã—ã„è¨“ç·´æˆ¦ç•¥ã‚’ç‹¬ç«‹ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦è¿½åŠ å¯èƒ½
- ä¿å®ˆæ€§: å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒç‰¹å®šã®è²¬å‹™ã«é›†ä¸­

**å°†æ¥ã®æ‹¡å¼µä¾‹**:
```python
# æ–°ã—ã„è¨“ç·´æˆ¦ç•¥ã‚’è¿½åŠ ã™ã‚‹å ´åˆ
src/ease/
â”œâ”€â”€ trainer.py      # ã‚³ã‚¢ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆå¤‰æ›´ä¸è¦ï¼‰
â”œâ”€â”€ ashem.py        # ASHEMæˆ¦ç•¥
â””â”€â”€ new_strategy.py # æ–°ã—ã„æˆ¦ç•¥ï¼ˆç‹¬ç«‹ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰
```

---

## å®Ÿé¨“å®Ÿè¡ŒåŸå‰‡

### Google Colabå®Ÿè¡Œã‚’å‰æ

**é‡è¦**: ã—ã°ã‚‰ãã®é–“ã€ã™ã¹ã¦ã®å®Ÿé¨“ã¯Google Colabã§å®Ÿè¡Œã—ã¾ã™ã€‚

#### ç†ç”±
- GPUï¼ˆNVIDIA L4ç­‰ï¼‰ã®åˆ©ç”¨å¯èƒ½æ€§
- å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆWikiText-2ç­‰ï¼‰ã®é«˜é€Ÿå‡¦ç†
- é•·æ™‚é–“è¨“ç·´ã®å®‰å®šå®Ÿè¡Œ

#### å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- **ãƒ¡ã‚¤ãƒ³å®Ÿé¨“**: `colab2.py` (ASHEMå®Ÿé¨“ã€fc9b140ã§å‹•ä½œç¢ºèªæ¸ˆã¿)
- ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å‰Šé™¤æ¸ˆã¿

#### Colabå®Ÿè¡Œæ™‚ã®æ³¨æ„ç‚¹

**ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼**:
```python
# datasets ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¿…è¦
!pip install datasets

# è‡ªå‹•çš„ã«Hugging Faceã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
from experiments.utils import create_wikitext_dataloaders
```

**GPUç¢ºèª**:
```python
import torch
print(f"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
```

**å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰**:
```bash
# Colabã‚»ãƒ«ã§å®Ÿè¡Œ
!python colab2.py
```

#### Gitæ“ä½œï¼ˆå‹•ä½œã™ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸ã®åˆ‡ã‚Šæ›¿ãˆï¼‰
```bash
# å‹•ä½œç¢ºèªæ¸ˆã¿ã®ã‚³ãƒŸãƒƒãƒˆã«åˆ‡ã‚Šæ›¿ãˆ
git checkout fc9b140

# ã¾ãŸã¯ã€æœ€æ–°ã®mainãƒ–ãƒ©ãƒ³ãƒã‚’ä½¿ç”¨ï¼ˆfc9b140ã¨åŒã˜ï¼‰
git checkout main
```

---

## è«–æ–‡æŠ•ç¨¿æ–¹é‡

### æŠ•ç¨¿å…ˆ

**ç¢ºå®š**: arXivï¼ˆçµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯è«–æ–‡ã¨ã—ã¦æŠ•ç¨¿ï¼‰

### è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«æ¡ˆ

**ãƒ¡ã‚¤ãƒ³**: LASH: Layered Adaptive Supervision Hierarchy for Efficient Transformer Training

**ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«**: A Unified Framework with 2 Core Options

### æ–°è¦æ€§ï¼ˆNoveltyï¼‰ã®ä¸»å¼µ

**å‚è€ƒæ–‡çŒ®**:
- ACM Survey (Nov 2024): "Early-Exit Deep Neural Networkâ€”A Comprehensive Survey" (37 pages)
- NLP Survey (Jan 2025): "A Survey of Early Exit Deep Neural Networks in NLP" (13 pages)

#### 1. çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ã—ã¦ã®æ–°è¦æ€§

**æ—¢å­˜ç ”ç©¶ã®å•é¡Œç‚¹**:
- Deep Supervisionã€Early Exitã¯å€‹åˆ¥ã«ææ¡ˆã•ã‚ŒãŸ
- ã“ã‚Œã‚‰ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã«ã¯åˆ¥ã€…ã®å®Ÿè£…ãŒå¿…è¦
- æŸ”è»Ÿãªæˆ¦ç•¥ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãŒå›°é›£

**LASHã®è²¢çŒ®**:
- 2ã¤ã®ã‚³ã‚¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆ`layer_weights`, `routing_threshold`ï¼‰ã§å…¨ã¦ã®æˆ¦ç•¥ã‚’çµ±ä¸€çš„ã«å®Ÿç¾
- å˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§3ã¤ä»¥ä¸Šã®è¨“ç·´æˆ¦ç•¥ã‚’ã‚µãƒãƒ¼ãƒˆ
- ç„¡é™ã®æˆ¦ç•¥ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãŒå¯èƒ½

**å„ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ–°è¦æ€§åˆ†æ**:

1. **`layer_weights` (å±¤ã”ã¨ã®æå¤±é‡ã¿)**:
   - âš ï¸ æ—¢å­˜ç ”ç©¶ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ï¼ˆä¸¡ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ã§ç¢ºèªï¼‰
   - æœ€ã‚‚ä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ `wi = i`ï¼ˆæ·±ã„å±¤ã»ã©é‡ã¿ãŒå¤§ãã„ï¼‰
   - âœ… **LASHã®ç‹¬è‡ªæ€§**: ä»»æ„ã®éå¯¾ç§°ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¯èƒ½ï¼ˆä¾‹: `{1: 0.7, 2: 0, 3: 0.3}`ï¼‰
   - âœ… ã‚¼ãƒ­é‡ã¿ã«ã‚ˆã‚‹å±¤ã®ã‚¹ã‚­ãƒƒãƒ—ãŒå¯èƒ½

2. **`routing_threshold` (Early Exité–¾å€¤)**:
   - âš ï¸ Early Exitè‡ªä½“ã¯æ—¢å­˜æŠ€è¡“ï¼ˆTeerapittayanon et al., 2016 - BranchyNetä»¥é™ï¼‰
   - âœ… **LASHã®ç‹¬è‡ªæ€§**: layer_weightsã¨ã®çµ±åˆã«ã‚ˆã‚‹æŸ”è»Ÿãªåˆ¶å¾¡

**ä¿®æ­£ã•ã‚ŒãŸClaim**:
"While existing work uses layer-wise loss weights with simple patterns (typically wi=i) [Survey'24], LASH is the first framework to simultaneously integrate:
1) Arbitrary asymmetric layer-wise supervision patterns
2) Early exit mechanisms with flexible control
through two independent, composable configuration parameters."

#### 2. ASHEMè¨“ç·´æˆ¦ç•¥ã®æ–°è¦æ€§

**æ—¢å­˜ç ”ç©¶ã¨ã®å·®åˆ¥åŒ–**:
- HAM/HSM: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åˆ†é‡ã®Hard example miningï¼ˆCV/NLPåˆ†é‡ã¨ã¯ç•°ãªã‚‹ï¼‰
- PLD: Progressive layer additionï¼ˆHard example miningã¨ã®çµ±åˆãªã—ï¼‰
- **ä¸¡ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡**: Early Exitã¨Hard Example Miningã®çµ„ã¿åˆã‚ã›ã«é–¢ã™ã‚‹è¨˜è¿°ãªã—

**ASHEMã®ç‹¬è‡ªæ€§**:
- âœ… **Strong Novelty**: Hard Example Mining + Early Exitã®çµ±åˆ
- âœ… Two-Phase Trainingï¼ˆæµ…å±¤â†’æ·±å±¤ã¸ã®æ®µéšçš„å±•é–‹ï¼‰
- âœ… Two-Stage Inferenceï¼ˆEarly Exitï¼‰ã¨ã®çµ„ã¿åˆã‚ã›
- âœ… è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¸ã®é©ç”¨ï¼ˆæ—¢å­˜ç ”ç©¶ã¯ä¸»ã«CVåˆ†é‡ï¼‰

**ä¿®æ­£ã•ã‚ŒãŸClaim**:
"ASHEM introduces a novel two-phase training paradigm that:
1) Trains a shallow model on all data
2) Selectively expands to deeper architecture trained exclusively on hard examples identified via confidence thresholds
3) Employs two-stage inference for computational efficiency

This is the first method to combine hard example mining with selective layer expansion and early exit for language modeling."

**æ³¨æ„**: "Progressive Layer Addition"ã¨ã„ã†ç”¨èªã¯PLD (NeurIPS 2020)ã¨æ··åŒã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€"Selective Layer Expansion"ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã€‚

---

### æ–°è¦æ€§è©•ä¾¡ã®ç·æ‹¬

**âœ… ç¢ºèªã•ã‚ŒãŸå¼·ã„æ–°è¦æ€§**:
1. **Hard Example Mining + Selective Layer Expansion + Early Exit**
   - ä¸¡ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ã§çµ„ã¿åˆã‚ã›ã«é–¢ã™ã‚‹è¨˜è¿°ãªã—
   - è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¸ã®é©ç”¨ã¯æœ¬ç ”ç©¶ãŒåˆã‚ã¦
   - Two-Phase Trainingï¼ˆæµ…å±¤â†’æ·±å±¤ã¸ã®æ®µéšçš„å±•é–‹ï¼‰

2. **2ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ±åˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**
   - ç‹¬ç«‹ã‹ã¤çµ„ã¿åˆã‚ã›å¯èƒ½ãª2ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã‚‹çµ±ä¸€çš„åˆ¶å¾¡
   - æ—¢å­˜æ‰‹æ³•ã¯å€‹åˆ¥å®Ÿè£…ãŒå¿…è¦
   - ä»»æ„ã®éå¯¾ç§°å±¤é‡ã¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Ÿç¾

**âš ï¸ æ—¢å­˜æŠ€è¡“ã‚’å«ã‚€è¦ç´ **:
1. **Layer-wise Loss Weights**: æ—¢å­˜ç ”ç©¶ã§ä½¿ç”¨æ¸ˆã¿ï¼ˆãŸã ã—ä»»æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ–°è¦ï¼‰
2. **Early Exit**: 2016å¹´ã‹ã‚‰ç¢ºç«‹ã•ã‚ŒãŸæŠ€è¡“ï¼ˆãŸã ã—çµ±åˆæ–¹æ³•ã¯æ–°è¦ï¼‰

**ğŸ“Š å®Ÿé¨“çš„æ¤œè¨¼**:
- WikiText-2 (10K samples)ã§ã®å®šé‡çš„æˆæœ
- Hard examplesã¸ã®é¡•è‘—ãªæ”¹å–„åŠ¹æœï¼ˆ78% PPLæ”¹å–„ï¼‰
- è¨ˆç®—åŠ¹ç‡ã¨ç²¾åº¦ã®ä¸¡ç«‹ã‚’å®Ÿè¨¼

---

#### 3. è‡ªå‹•æœ€é©åŒ–ã®æ–°è¦æ€§

**LASHã®è²¢çŒ®**:
- `layer_weights`ã‚’è§£æã—ã€æœ€é©ãªå®Ÿè¡Œãƒ‘ã‚¹ã‚’è‡ªå‹•é¸æŠ
- æœ€çµ‚å±¤ã®ã¿ã®å ´åˆã€8.4%ã®è¨“ç·´é€Ÿåº¦å‘ä¸Š
- ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®æŸ”è»Ÿæ€§ã‚’æãªã‚ãªã„æœ€é©åŒ–

**Claim**: "LASH automatically optimizes execution paths based on layer weight configuration, achieving 8.4% speedup while maintaining full flexibility."

#### 4. å®Ÿé¨“çµæœã®æ–°è¦æ€§

**WikiText-2ã§ã®æ¤œè¨¼çµæœ**ï¼ˆ10K samples, fc9b140ï¼‰:
- Hard PPL: **78%æ”¹å–„**ï¼ˆ2763 â†’ 668ï¼‰
- è¨ˆç®—ã‚³ã‚¹ãƒˆ: **36%å‰Šæ¸›**ï¼ˆ64.82% of full modelï¼‰
- Overall PPL: **15.9%æ”¹å–„**ï¼ˆ986 â†’ 830ï¼‰
- Overall Accuracy: 16.03% â†’ 15.77%ï¼ˆå¾®æ¸›ï¼‰

**æ—¢å­˜ç ”ç©¶ã¨ã®å·®åˆ¥åŒ–**:
- **Deep Supervision**: å…¨å±¤ã§è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆåŠ¹ç‡æ€§ã«èª²é¡Œï¼‰
- **Early Exit**: è¨“ç·´æˆ¦ç•¥ã¯å¾“æ¥å‹ã®ã¾ã¾ï¼ˆHard examplesã¸ã®å¯¾å¿œãªã—ï¼‰
- **ASHEM**: è¨“ç·´ã¨æ¨è«–ã®ä¸¡æ–¹ã‚’æœ€é©åŒ–ï¼ˆHard examplesã«ç‰¹åŒ–ã—ãŸæ®µéšçš„è¨“ç·´ï¼‰

**é‡è¦ãªçŸ¥è¦‹**:
- Hard examplesã¸ã®ç‰¹åŒ–è¨“ç·´ã«ã‚ˆã‚Šã€é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã§ã®æ€§èƒ½ãŒå¤§å¹…å‘ä¸Š
- Early Exitã«ã‚ˆã‚‹æ¨è«–æ™‚ã®è¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›
- å…¨ä½“ã®ç²¾åº¦ã‚’ç¶­æŒã—ãªãŒã‚‰åŠ¹ç‡ã‚’æ”¹å–„

### è«–æ–‡æ§‹æˆæ¡ˆ

1. **Introduction**: çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å¿…è¦æ€§
   - æ—¢å­˜æ‰‹æ³•ã®å€‹åˆ¥å®Ÿè£…ã®èª²é¡Œ
   - æŸ”è»Ÿãªæˆ¦ç•¥ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã®é‡è¦æ€§

2. **Related Work**:
   - **Deep Supervision** (Lee et al., 2015)
   - **Discriminative Fine-Tuning** (Howard & Ruder, 2018)
   - **Early Exit Networks** (Teerapittayanon et al., 2016; BranchyNet)
   - **Hard Example Mining** (HAM, HSMç­‰ - ä¸»ã«CV/ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åˆ†é‡)
   - **Recent Surveys** (ACM Survey Nov 2024, NLP Survey Jan 2025)
   - **æ—¢å­˜æ‰‹æ³•ã®èª²é¡Œ**: å€‹åˆ¥å®Ÿè£…ã€çµ±åˆã®å›°é›£ã•

3. **LASH Framework**: 2ã¤ã®ã‚³ã‚¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
   - `layer_weights`: ä»»æ„ã®éå¯¾ç§°ãƒ‘ã‚¿ãƒ¼ãƒ³
   - `routing_threshold`: Early Exité–¾å€¤
   - è‡ªå‹•æœ€é©åŒ–æ©Ÿæ§‹

4. **ASHEM Training Strategy**: Hard example miningã‚’æ´»ç”¨ã—ãŸæ–°ã—ã„è¨“ç·´æˆ¦ç•¥
   - Two-Phase Trainingï¼ˆæµ…å±¤â†’æ·±å±¤ï¼‰
   - Hard Example Identificationï¼ˆPer-token filteringï¼‰
   - Two-Stage Inference

5. **Experiments**: WikiText-2/103ã§ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ
   - Standard vs Deep Supervision vs ASHEM
   - Hard examples vs Easy examples ã®åˆ†æ
   - è¨ˆç®—ã‚³ã‚¹ãƒˆã¨ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

6. **Analysis**:
   - Ablation studyï¼ˆASHEMã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼‰
   - Thresholdæ„Ÿåº¦åˆ†æ
   - è¨ˆç®—åŠ¹ç‡åˆ†æï¼ˆFLOPs, wall-clock timeï¼‰
   - Scalabilityæ¤œè¨¼

7. **Conclusion**: çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®æ„ç¾©ã¨å°†æ¥å±•æœ›
   - æ–°è¦æ€§ã®å†ç¢ºèª
   - å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¸ã®å±•é–‹å¯èƒ½æ€§

### ä»Šå¾Œã®å®Ÿé¨“è¨ˆç”»

#### Tier 1ï¼ˆå¿…é ˆå®Ÿé¨“ï¼‰

- [ ] WikiText-103ã§ã®æ¤œè¨¼ï¼ˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ï¼‰
- [ ] ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒï¼ˆStandard, Deep Supervision, Discriminative FT, Early Exitï¼‰
- [ ] ä¸­è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆdim=128, layers=4â†’6ï¼‰ã§ã®æ¤œè¨¼

#### Tier 2ï¼ˆå¼·ãæ¨å¥¨ï¼‰

- [ ] Ablation Studyï¼ˆASHEMã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼‰
- [ ] Thresholdæ„Ÿåº¦åˆ†æï¼ˆ0.7, 0.8, 0.9, 0.95, 0.99ï¼‰
- [ ] è¨ˆç®—åŠ¹ç‡åˆ†æï¼ˆFLOPs, wall-clock timeï¼‰

#### Tier 3ï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰

- [ ] å®Ÿéš›ã®LLMï¼ˆLlamaç­‰ï¼‰ã§ã®æ¤œè¨¼
- [ ] ä»–ã®ã‚¿ã‚¹ã‚¯ï¼ˆåˆ†é¡ã€è¦ç´„ç­‰ï¼‰ã¸ã®é©ç”¨
- [ ] å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆC4, The Pileç­‰ï¼‰ã§ã®æ¤œè¨¼

---

## ä»Šå¾Œã®ã‚¿ã‚¹ã‚¯

- [ ] ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã§ã®æ¤œè¨¼å®Ÿé¨“ï¼ˆdim=128, layers=6ï¼‰
- [ ] å®Ÿéš›ã® LLM (Llama ç­‰) ã§ã®æ¤œè¨¼
- [ ] ASHEMä»¥å¤–ã®æ–°ã—ã„è¨“ç·´æˆ¦ç•¥ã®é–‹ç™º
- [ ] ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆC4, The Pileç­‰ï¼‰ã§ã®æ¤œè¨¼
- [ ] Staged DS ã®å®Ÿè£…å®Œæˆï¼ˆPer-token filtering ã®æ­£ã—ã„å®Ÿè£…ï¼‰
